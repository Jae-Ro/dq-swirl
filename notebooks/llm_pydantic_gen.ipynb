{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24ebfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, constr, field_validator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "from dq_swirl.clients.async_llm_client import AsyncLLMClient\n",
    "from dq_swirl.ingestion.structure_analyzer import StructuralAnalyzer\n",
    "from dq_swirl.rust_ingestion import smart_parse_batch\n",
    "\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6168d974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../secrets.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067ba83",
   "metadata": {},
   "source": [
    "## Messy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9549bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_data = [\n",
    "    \"Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
    "    \"Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp\",\n",
    "    \"Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable\",\n",
    "    \"Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard\",\n",
    "    \"Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]\",\n",
    "    \"Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \",\n",
    "    \"Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
    "    \"Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones\",\n",
    "    \"Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse\",\n",
    "    \"Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker\",\n",
    "    \"Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp\",\n",
    "    \"Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,\",\n",
    "    '{\"id\": \"usr_001\", \"name\": \"Alex Johnson\", \"role\": \"admin\", \"isActive\": true, \"createdAt\": \"2025-11-02T09:14:23Z\"}',\n",
    "    '{\"id\": \"usr_002\", \"name\": \"Maria Lopez\", \"email\": \"maria.lopez@example.com\", \"role\": \"editor\", \"isActive\": null, \"createdAt\": \"2025-12-18T16:47:10Z\", \"lastLoginIp\": \"192.168.1.42\"}',\n",
    "    '{\"id\": \"usr_003\", \"email\": \"samir.patel@example.com\", \"role\": \"viewer\", \"isActive\": false, \"createdAt\": \"08/05/2024\"}',\n",
    "    '{\"id\": 4, \"name\": \"Chen Wei\", \"email\": \"chen.wei@example.com\", \"isActive\": true, \"createdAt\": null}',\n",
    "    '{\"id\": \"usr_005\", \"name\": \"Broken Record\", \"email\": \"broken@example.com\"}',\n",
    "    \"Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes\",\n",
    "    \"Order=1018, Buyer=Jae Arora, Location=Dreher, FL, Total=$6.00, Items: chair, Discount: true, phone=123-456-789\",\n",
    "    \"Order=1019, Buyer=Jae Kao, Location=Atlanta, GA, Total=$12.00, Items: desk, Discount: False, phone=123-456-789\",\n",
    "    \"2026-01-30 14:22:01 INFO User login successful user_id=123\",\n",
    "    \"2026-01-30 14:22:01 INFO User login successful\",\n",
    "    \"level =INFO, user =Sam, id=1\",\n",
    "    \"timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true\",\n",
    "    \"level=INFO cpu_usage=1,234.56 memory=512MB\",\n",
    "    '{\"level\":\"INFO\",\"service\":\"orders\",\"order_id\":1001,\"status\":\"created\"}',\n",
    "    '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"',\n",
    "    \"2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d865678",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be705a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UNSTRUCTURED STRING SAMPLES: 22\n",
      "\n",
      "JSON STRING SAMPLES: 6\n",
      "\n",
      "Original: Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\n",
      "Parsed: {'order': '1001', 'buyer': 'John Davis', 'location': 'Columbus, OH', 'total': '$742.10', 'items': 'laptop, hdmi cable'}\n",
      "\n",
      "Original: Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp\n",
      "Parsed: {'order': '1004', 'buyer': 'AMANDA SMITH', 'location': 'Seattle, WA', 'total': '$50.00', 'items': 'desk lamp'}\n",
      "\n",
      "Original: Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable\n",
      "Parsed: {'order': '1005', 'buyer': 'Raj Patel', 'total': '1,200.50', 'items': 'monitor, stand, cable'}\n",
      "\n",
      "Original: Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard\n",
      "Parsed: {'order': '1006', 'total': '$89.99', 'location': 'Miami, FL', 'buyer': 'Elena Rossi', 'items': 'keyboard'}\n",
      "\n",
      "Original: Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]\n",
      "Parsed: {'order': '1007', 'buyer': 'Chris P.', 'location': 'Denver, CO', 'total': '$12.00', 'items': 'stickers -- [DISCOUNT APPLIED]'}\n",
      "\n",
      "Original: Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \n",
      "Parsed: {'order': '1008', 'buyer': \"O'Connor, S.\", 'location': 'Portland, OR', 'total': '$0.00', 'items': 'None'}\n",
      "\n",
      "Original: Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\n",
      "Parsed: {'order': '1011', 'buyer': 'John Davis', 'location': 'Columbus, OH', 'total': '$742.10', 'items': 'laptop, hdmi cable'}\n",
      "\n",
      "Original: Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones\n",
      "Parsed: {'order': '1012', 'buyer': 'Sarah Liu', 'location': 'Austin, TX', 'total': '$156.55', 'items': 'headphones'}\n",
      "\n",
      "Original: Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse\n",
      "Parsed: {'order': '1013', 'buyer': 'Mike Turner', 'location': 'Cleveland, OH', 'total': '$1299.99', 'items': 'gaming pc, mouse'}\n",
      "\n",
      "Original: Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker\n",
      "Parsed: {'order': '1014', 'buyer': 'Rachel Kim', 'locadtion': 'Seattle, WA', 'total': '$89.50', 'items': 'coffee maker'}\n",
      "\n",
      "Original: Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp\n",
      "Parsed: {'order': '1015', 'buyer': 'Chris Myers', 'location': 'Cincinnati, OH', 'total': '$512.00', 'items': 'monitor, desk lamp'}\n",
      "\n",
      "Original: Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,\n",
      "Parsed: {'order': '1016', 'buyer': 'Jake Myers', 'total': '$1,512.00', 'items': 'monitor'}\n",
      "\n",
      "Original: Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes\n",
      "Parsed: {'order': '1017', 'buyer': 'Griffin Arora', 'location': 'Columbia, SC', 'total': '$512.00', 'items': 'monitor, desk lamp', 'discount': 'yes'}\n",
      "\n",
      "Original: Order=1018, Buyer=Jae Arora, Location=Dreher, FL, Total=$6.00, Items: chair, Discount: true, phone=123-456-789\n",
      "Parsed: {'order': '1018', 'buyer': 'Jae Arora', 'location': 'Dreher, FL', 'total': '$6.00', 'items': 'chair', 'discount': 'true', 'phone': '123-456-789'}\n",
      "\n",
      "Original: Order=1019, Buyer=Jae Kao, Location=Atlanta, GA, Total=$12.00, Items: desk, Discount: False, phone=123-456-789\n",
      "Parsed: {'order': '1019', 'buyer': 'Jae Kao', 'location': 'Atlanta, GA', 'total': '$12.00', 'items': 'desk', 'discount': 'False', 'phone': '123-456-789'}\n",
      "\n",
      "Original: 2026-01-30 14:22:01 INFO User login successful user_id=123\n",
      "Parsed: {'user_id': '123', '_unparsed': '2026-01-30 14:22:01 INFO User login successful'}\n",
      "\n",
      "Original: 2026-01-30 14:22:01 INFO User login successful\n",
      "Parsed: {'_unparsed': '2026-01-30 14:22:01 INFO User login successful'}\n",
      "\n",
      "Original: level =INFO, user =Sam, id=1\n",
      "Parsed: {'level': 'INFO', 'user': 'Sam', 'id': '1'}\n",
      "\n",
      "Original: timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true\n",
      "Parsed: {'timestamp': '2026-01-30T14:22:01Z', 'level': 'INFO', 'user': 'alice', 'action': 'login', 'success': 'true'}\n",
      "\n",
      "Original: level=INFO cpu_usage=1,234.56 memory=512MB\n",
      "Parsed: {'level': 'INFO', 'cpu_usage': '1,234.56', 'memory': '512MB'}\n",
      "\n",
      "Original: [2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"\n",
      "Parsed: {'_unparsed': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"'}\n",
      "\n",
      "Original: 2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\n",
      "Parsed: {'_unparsed': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check'}\n",
      "\n",
      "\n",
      "TOTAL SAMPLES: 28\n",
      "ERROR SAMPLES: 0\n",
      "\n",
      "Detected 16 unique schemas across 28 records.\n",
      "\n",
      "Schema 28d9f3b14d0e5516a186062212502d0c (1 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'locadtion': 'str', 'total': 'str', 'items': 'str'}\n",
      "------------------------------\n",
      "Schema 2bcfb738a2056c5ef5543917c9f3ac53 (1 occurrences):\n",
      "  Layout: {'id': 'int', 'name': 'str', 'email': 'str', 'isactive': 'bool', 'createdat': 'NoneType'}\n",
      "------------------------------\n",
      "Schema 3baae1f59cac077e89e2f0b7d47a36cf (1 occurrences):\n",
      "  Layout: {'user_id': 'str', '_unparsed': 'str'}\n",
      "------------------------------\n",
      "Schema 4286ed8928a42e13682233f00079d106 (1 occurrences):\n",
      "  Layout: {'id': 'str', 'name': 'str', 'email': 'str', 'role': 'str', 'isactive': 'NoneType', 'createdat': 'str', 'lastloginip': 'str'}\n",
      "------------------------------\n",
      "Schema 461a895ef9c5046dd2cb5026b6a62de0 (1 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'location': 'str', 'total': 'str', 'items': 'str', 'discount': 'str'}\n",
      "------------------------------\n",
      "Schema 50eb97a85647221ecc7f65f74d68d156 (2 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'total': 'str', 'items': 'str'}\n",
      "------------------------------\n",
      "Schema 7299d10d006e4a3fc143b9d77e79917e (2 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'location': 'str', 'total': 'str', 'items': 'str', 'discount': 'str', 'phone': 'str'}\n",
      "------------------------------\n",
      "Schema 74db6b18a3e440d4dd054614492fa510 (1 occurrences):\n",
      "  Layout: {'timestamp': 'str', 'level': 'str', 'user': 'str', 'action': 'str', 'success': 'str'}\n",
      "------------------------------\n",
      "Schema a0713c61038426204fef9da85c91686f (1 occurrences):\n",
      "  Layout: {'id': 'str', 'name': 'str', 'role': 'str', 'isactive': 'bool', 'createdat': 'str'}\n",
      "------------------------------\n",
      "Schema c2aebc126ff252eb9dae084d659d2cab (1 occurrences):\n",
      "  Layout: {'level': 'str', 'user': 'str', 'id': 'str'}\n",
      "------------------------------\n",
      "Schema d0f1c5f0782666cd8324b4cf57a3159d (1 occurrences):\n",
      "  Layout: {'level': 'str', 'cpu_usage': 'str', 'memory': 'str'}\n",
      "------------------------------\n",
      "Schema d2d16f7c3698c6195ddaeb6205139150 (1 occurrences):\n",
      "  Layout: {'id': 'str', 'name': 'str', 'email': 'str'}\n",
      "------------------------------\n",
      "Schema d5d811706d95e2f6c1b713aa3fd1d9ab (1 occurrences):\n",
      "  Layout: {'id': 'str', 'email': 'str', 'role': 'str', 'isactive': 'bool', 'createdat': 'str'}\n",
      "------------------------------\n",
      "Schema df87355cb94200d773396f5befa867d4 (3 occurrences):\n",
      "  Layout: {'_unparsed': 'str'}\n",
      "------------------------------\n",
      "Schema e8b13b0b419896ad0f402c15a762e2c1 (1 occurrences):\n",
      "  Layout: {'level': 'str', 'service': 'str', 'order_id': 'int', 'status': 'str'}\n",
      "------------------------------\n",
      "Schema fd116cd512d5ecd2e59edf12fc258b32 (9 occurrences):\n",
      "  Layout: {'order': 'str', 'buyer': 'str', 'location': 'str', 'total': 'str', 'items': 'str'}\n",
      "------------------------------\n",
      "Structural Clusters: \n",
      "{\n",
      "    \"1\": [\n",
      "        {\n",
      "            \"signature_hash\": \"28d9f3b14d0e5516a186062212502d0c\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"locadtion\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"461a895ef9c5046dd2cb5026b6a62de0\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\",\n",
      "                \"discount\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"50eb97a85647221ecc7f65f74d68d156\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"7299d10d006e4a3fc143b9d77e79917e\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\",\n",
      "                \"discount\",\n",
      "                \"phone\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"fd116cd512d5ecd2e59edf12fc258b32\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"0\": [\n",
      "        {\n",
      "            \"signature_hash\": \"2bcfb738a2056c5ef5543917c9f3ac53\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"4286ed8928a42e13682233f00079d106\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\",\n",
      "                \"lastloginip\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"a0713c61038426204fef9da85c91686f\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d2d16f7c3698c6195ddaeb6205139150\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d5d811706d95e2f6c1b713aa3fd1d9ab\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"email\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"3\": [\n",
      "        {\n",
      "            \"signature_hash\": \"3baae1f59cac077e89e2f0b7d47a36cf\",\n",
      "            \"fields\": [\n",
      "                \"user_id\",\n",
      "                \"_unparsed\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"df87355cb94200d773396f5befa867d4\",\n",
      "            \"fields\": [\n",
      "                \"_unparsed\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"e8b13b0b419896ad0f402c15a762e2c1\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"service\",\n",
      "                \"order_id\",\n",
      "                \"status\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"2\": [\n",
      "        {\n",
      "            \"signature_hash\": \"74db6b18a3e440d4dd054614492fa510\",\n",
      "            \"fields\": [\n",
      "                \"timestamp\",\n",
      "                \"level\",\n",
      "                \"user\",\n",
      "                \"action\",\n",
      "                \"success\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"c2aebc126ff252eb9dae084d659d2cab\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"user\",\n",
      "                \"id\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d0f1c5f0782666cd8324b4cf57a3159d\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"cpu_usage\",\n",
      "                \"memory\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n",
      "{\n",
      "    \"fd116cd512d5ecd2e59edf12fc258b32\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"location\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1001\",\n",
      "                    \"buyer\": \"John Davis\",\n",
      "                    \"location\": \"Columbus, OH\",\n",
      "                    \"total\": \"$742.10\",\n",
      "                    \"items\": \"laptop, hdmi cable\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1004\",\n",
      "                    \"buyer\": \"AMANDA SMITH\",\n",
      "                    \"location\": \"Seattle, WA\",\n",
      "                    \"total\": \"$50.00\",\n",
      "                    \"items\": \"desk lamp\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1006\",\n",
      "                    \"total\": \"$89.99\",\n",
      "                    \"location\": \"Miami, FL\",\n",
      "                    \"buyer\": \"Elena Rossi\",\n",
      "                    \"items\": \"keyboard\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1007\",\n",
      "                    \"buyer\": \"Chris P.\",\n",
      "                    \"location\": \"Denver, CO\",\n",
      "                    \"total\": \"$12.00\",\n",
      "                    \"items\": \"stickers -- [DISCOUNT APPLIED]\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1008\",\n",
      "                    \"buyer\": \"O'Connor, S.\",\n",
      "                    \"location\": \"Portland, OR\",\n",
      "                    \"total\": \"$0.00\",\n",
      "                    \"items\": \"None\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1011\",\n",
      "                    \"buyer\": \"John Davis\",\n",
      "                    \"location\": \"Columbus, OH\",\n",
      "                    \"total\": \"$742.10\",\n",
      "                    \"items\": \"laptop, hdmi cable\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1012\",\n",
      "                    \"buyer\": \"Sarah Liu\",\n",
      "                    \"location\": \"Austin, TX\",\n",
      "                    \"total\": \"$156.55\",\n",
      "                    \"items\": \"headphones\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1013\",\n",
      "                    \"buyer\": \"Mike Turner\",\n",
      "                    \"location\": \"Cleveland, OH\",\n",
      "                    \"total\": \"$1299.99\",\n",
      "                    \"items\": \"gaming pc, mouse\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1015\",\n",
      "                    \"buyer\": \"Chris Myers\",\n",
      "                    \"location\": \"Cincinnati, OH\",\n",
      "                    \"total\": \"$512.00\",\n",
      "                    \"items\": \"monitor, desk lamp\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"50eb97a85647221ecc7f65f74d68d156\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1005\",\n",
      "                    \"buyer\": \"Raj Patel\",\n",
      "                    \"total\": \"1,200.50\",\n",
      "                    \"items\": \"monitor, stand, cable\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1016\",\n",
      "                    \"buyer\": \"Jake Myers\",\n",
      "                    \"total\": \"$1,512.00\",\n",
      "                    \"items\": \"monitor\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"28d9f3b14d0e5516a186062212502d0c\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"locadtion\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1014\",\n",
      "                    \"buyer\": \"Rachel Kim\",\n",
      "                    \"locadtion\": \"Seattle, WA\",\n",
      "                    \"total\": \"$89.50\",\n",
      "                    \"items\": \"coffee maker\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"461a895ef9c5046dd2cb5026b6a62de0\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"location\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\",\n",
      "            \"discount\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1017\",\n",
      "                    \"buyer\": \"Griffin Arora\",\n",
      "                    \"location\": \"Columbia, SC\",\n",
      "                    \"total\": \"$512.00\",\n",
      "                    \"items\": \"monitor, desk lamp\",\n",
      "                    \"discount\": \"yes\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"7299d10d006e4a3fc143b9d77e79917e\": {\n",
      "        \"signature\": {\n",
      "            \"order\": \"str\",\n",
      "            \"buyer\": \"str\",\n",
      "            \"location\": \"str\",\n",
      "            \"total\": \"str\",\n",
      "            \"items\": \"str\",\n",
      "            \"discount\": \"str\",\n",
      "            \"phone\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"Order=1018, Buyer=Jae Arora, Location=Dreher, FL, Total=$6.00, Items: chair, Discount: true, phone=123-456-789\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1018\",\n",
      "                    \"buyer\": \"Jae Arora\",\n",
      "                    \"location\": \"Dreher, FL\",\n",
      "                    \"total\": \"$6.00\",\n",
      "                    \"items\": \"chair\",\n",
      "                    \"discount\": \"true\",\n",
      "                    \"phone\": \"123-456-789\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"Order=1019, Buyer=Jae Kao, Location=Atlanta, GA, Total=$12.00, Items: desk, Discount: False, phone=123-456-789\",\n",
      "                \"parsed\": {\n",
      "                    \"order\": \"1019\",\n",
      "                    \"buyer\": \"Jae Kao\",\n",
      "                    \"location\": \"Atlanta, GA\",\n",
      "                    \"total\": \"$12.00\",\n",
      "                    \"items\": \"desk\",\n",
      "                    \"discount\": \"False\",\n",
      "                    \"phone\": \"123-456-789\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"3baae1f59cac077e89e2f0b7d47a36cf\": {\n",
      "        \"signature\": {\n",
      "            \"user_id\": \"str\",\n",
      "            \"_unparsed\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"2026-01-30 14:22:01 INFO User login successful user_id=123\",\n",
      "                \"parsed\": {\n",
      "                    \"user_id\": \"123\",\n",
      "                    \"_unparsed\": \"2026-01-30 14:22:01 INFO User login successful\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"df87355cb94200d773396f5befa867d4\": {\n",
      "        \"signature\": {\n",
      "            \"_unparsed\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"2026-01-30 14:22:01 INFO User login successful\",\n",
      "                \"parsed\": {\n",
      "                    \"_unparsed\": \"2026-01-30 14:22:01 INFO User login successful\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \\\"GET /health 1.1\\\" 200 16 \\\"-\\\" \\\"curl/8.14.1\\\"\",\n",
      "                \"parsed\": {\n",
      "                    \"_unparsed\": \"[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \\\"GET /health 1.1\\\" 200 16 \\\"-\\\" \\\"curl/8.14.1\\\"\"\n",
      "                }\n",
      "            },\n",
      "            {\n",
      "                \"raw\": \"2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\",\n",
      "                \"parsed\": {\n",
      "                    \"_unparsed\": \"2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"c2aebc126ff252eb9dae084d659d2cab\": {\n",
      "        \"signature\": {\n",
      "            \"level\": \"str\",\n",
      "            \"user\": \"str\",\n",
      "            \"id\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"level =INFO, user =Sam, id=1\",\n",
      "                \"parsed\": {\n",
      "                    \"level\": \"INFO\",\n",
      "                    \"user\": \"Sam\",\n",
      "                    \"id\": \"1\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"74db6b18a3e440d4dd054614492fa510\": {\n",
      "        \"signature\": {\n",
      "            \"timestamp\": \"str\",\n",
      "            \"level\": \"str\",\n",
      "            \"user\": \"str\",\n",
      "            \"action\": \"str\",\n",
      "            \"success\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true\",\n",
      "                \"parsed\": {\n",
      "                    \"timestamp\": \"2026-01-30T14:22:01Z\",\n",
      "                    \"level\": \"INFO\",\n",
      "                    \"user\": \"alice\",\n",
      "                    \"action\": \"login\",\n",
      "                    \"success\": \"true\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"d0f1c5f0782666cd8324b4cf57a3159d\": {\n",
      "        \"signature\": {\n",
      "            \"level\": \"str\",\n",
      "            \"cpu_usage\": \"str\",\n",
      "            \"memory\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"level=INFO cpu_usage=1,234.56 memory=512MB\",\n",
      "                \"parsed\": {\n",
      "                    \"level\": \"INFO\",\n",
      "                    \"cpu_usage\": \"1,234.56\",\n",
      "                    \"memory\": \"512MB\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"a0713c61038426204fef9da85c91686f\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"str\",\n",
      "            \"name\": \"str\",\n",
      "            \"role\": \"str\",\n",
      "            \"isactive\": \"bool\",\n",
      "            \"createdat\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": \\\"usr_001\\\", \\\"name\\\": \\\"Alex Johnson\\\", \\\"role\\\": \\\"admin\\\", \\\"isActive\\\": true, \\\"createdAt\\\": \\\"2025-11-02T09:14:23Z\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": \"usr_001\",\n",
      "                    \"name\": \"Alex Johnson\",\n",
      "                    \"role\": \"admin\",\n",
      "                    \"isActive\": true,\n",
      "                    \"createdAt\": \"2025-11-02T09:14:23Z\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"4286ed8928a42e13682233f00079d106\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"str\",\n",
      "            \"name\": \"str\",\n",
      "            \"email\": \"str\",\n",
      "            \"role\": \"str\",\n",
      "            \"isactive\": \"NoneType\",\n",
      "            \"createdat\": \"str\",\n",
      "            \"lastloginip\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": \\\"usr_002\\\", \\\"name\\\": \\\"Maria Lopez\\\", \\\"email\\\": \\\"maria.lopez@example.com\\\", \\\"role\\\": \\\"editor\\\", \\\"isActive\\\": null, \\\"createdAt\\\": \\\"2025-12-18T16:47:10Z\\\", \\\"lastLoginIp\\\": \\\"192.168.1.42\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": \"usr_002\",\n",
      "                    \"name\": \"Maria Lopez\",\n",
      "                    \"email\": \"maria.lopez@example.com\",\n",
      "                    \"role\": \"editor\",\n",
      "                    \"isActive\": null,\n",
      "                    \"createdAt\": \"2025-12-18T16:47:10Z\",\n",
      "                    \"lastLoginIp\": \"192.168.1.42\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"d5d811706d95e2f6c1b713aa3fd1d9ab\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"str\",\n",
      "            \"email\": \"str\",\n",
      "            \"role\": \"str\",\n",
      "            \"isactive\": \"bool\",\n",
      "            \"createdat\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": \\\"usr_003\\\", \\\"email\\\": \\\"samir.patel@example.com\\\", \\\"role\\\": \\\"viewer\\\", \\\"isActive\\\": false, \\\"createdAt\\\": \\\"08/05/2024\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": \"usr_003\",\n",
      "                    \"email\": \"samir.patel@example.com\",\n",
      "                    \"role\": \"viewer\",\n",
      "                    \"isActive\": false,\n",
      "                    \"createdAt\": \"08/05/2024\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"2bcfb738a2056c5ef5543917c9f3ac53\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"int\",\n",
      "            \"name\": \"str\",\n",
      "            \"email\": \"str\",\n",
      "            \"isactive\": \"bool\",\n",
      "            \"createdat\": \"NoneType\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": 4, \\\"name\\\": \\\"Chen Wei\\\", \\\"email\\\": \\\"chen.wei@example.com\\\", \\\"isActive\\\": true, \\\"createdAt\\\": null}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": 4,\n",
      "                    \"name\": \"Chen Wei\",\n",
      "                    \"email\": \"chen.wei@example.com\",\n",
      "                    \"isActive\": true,\n",
      "                    \"createdAt\": null\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"d2d16f7c3698c6195ddaeb6205139150\": {\n",
      "        \"signature\": {\n",
      "            \"id\": \"str\",\n",
      "            \"name\": \"str\",\n",
      "            \"email\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"id\\\": \\\"usr_005\\\", \\\"name\\\": \\\"Broken Record\\\", \\\"email\\\": \\\"broken@example.com\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"id\": \"usr_005\",\n",
      "                    \"name\": \"Broken Record\",\n",
      "                    \"email\": \"broken@example.com\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"e8b13b0b419896ad0f402c15a762e2c1\": {\n",
      "        \"signature\": {\n",
      "            \"level\": \"str\",\n",
      "            \"service\": \"str\",\n",
      "            \"order_id\": \"int\",\n",
      "            \"status\": \"str\"\n",
      "        },\n",
      "        \"records\": [\n",
      "            {\n",
      "                \"raw\": \"{\\\"level\\\":\\\"INFO\\\",\\\"service\\\":\\\"orders\\\",\\\"order_id\\\":1001,\\\"status\\\":\\\"created\\\"}\",\n",
      "                \"parsed\": {\n",
      "                    \"level\": \"INFO\",\n",
      "                    \"service\": \"orders\",\n",
      "                    \"order_id\": 1001,\n",
      "                    \"status\": \"created\"\n",
      "                }\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1872.56it/s, Materializing param=pooler.dense.weight]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Clusters: \n",
      "{\n",
      "    \"0\": [\n",
      "        {\n",
      "            \"signature_hash\": \"fd116cd512d5ecd2e59edf12fc258b32\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"50eb97a85647221ecc7f65f74d68d156\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"28d9f3b14d0e5516a186062212502d0c\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"locadtion\",\n",
      "                \"total\",\n",
      "                \"items\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"461a895ef9c5046dd2cb5026b6a62de0\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\",\n",
      "                \"discount\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"7299d10d006e4a3fc143b9d77e79917e\",\n",
      "            \"fields\": [\n",
      "                \"order\",\n",
      "                \"buyer\",\n",
      "                \"location\",\n",
      "                \"total\",\n",
      "                \"items\",\n",
      "                \"discount\",\n",
      "                \"phone\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"1\": [\n",
      "        {\n",
      "            \"signature_hash\": \"3baae1f59cac077e89e2f0b7d47a36cf\",\n",
      "            \"fields\": [\n",
      "                \"user_id\",\n",
      "                \"_unparsed\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"c2aebc126ff252eb9dae084d659d2cab\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"user\",\n",
      "                \"id\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"74db6b18a3e440d4dd054614492fa510\",\n",
      "            \"fields\": [\n",
      "                \"timestamp\",\n",
      "                \"level\",\n",
      "                \"user\",\n",
      "                \"action\",\n",
      "                \"success\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"406\": [\n",
      "        {\n",
      "            \"signature_hash\": \"df87355cb94200d773396f5befa867d4\",\n",
      "            \"fields\": [\n",
      "                \"_unparsed\"\n",
      "            ],\n",
      "            \"is_outlier\": true\n",
      "        }\n",
      "    ],\n",
      "    \"409\": [\n",
      "        {\n",
      "            \"signature_hash\": \"d0f1c5f0782666cd8324b4cf57a3159d\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"cpu_usage\",\n",
      "                \"memory\"\n",
      "            ],\n",
      "            \"is_outlier\": true\n",
      "        }\n",
      "    ],\n",
      "    \"2\": [\n",
      "        {\n",
      "            \"signature_hash\": \"a0713c61038426204fef9da85c91686f\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"4286ed8928a42e13682233f00079d106\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\",\n",
      "                \"lastloginip\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d5d811706d95e2f6c1b713aa3fd1d9ab\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"email\",\n",
      "                \"role\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"2bcfb738a2056c5ef5543917c9f3ac53\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\",\n",
      "                \"isactive\",\n",
      "                \"createdat\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        },\n",
      "        {\n",
      "            \"signature_hash\": \"d2d16f7c3698c6195ddaeb6205139150\",\n",
      "            \"fields\": [\n",
      "                \"id\",\n",
      "                \"name\",\n",
      "                \"email\"\n",
      "            ],\n",
      "            \"is_outlier\": false\n",
      "        }\n",
      "    ],\n",
      "    \"415\": [\n",
      "        {\n",
      "            \"signature_hash\": \"e8b13b0b419896ad0f402c15a762e2c1\",\n",
      "            \"fields\": [\n",
      "                \"level\",\n",
      "                \"service\",\n",
      "                \"order_id\",\n",
      "                \"status\"\n",
      "            ],\n",
      "            \"is_outlier\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaero/projects/interviews/team_raft/dq-swirl/.venv/lib/python3.14/site-packages/sklearn/cluster/_hdbscan/hdbscan.py:722: FutureWarning: The default value of `copy` will change from False to True in 1.10. Explicitly set a value for `copy` to silence this warning.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "################################# Grammar Parsing ###############################\n",
    "#################################################################################\n",
    "\n",
    "string_batch = []\n",
    "string_json_batch = []\n",
    "for msg in messy_data:\n",
    "    if not (msg.startswith(\"[\") and msg.endswith(\"]\")) and not (\n",
    "        msg.startswith(\"{\") and msg.endswith(\"}\")\n",
    "    ):\n",
    "        string_batch.append(msg)\n",
    "    else:\n",
    "        string_json_batch.append(msg)\n",
    "\n",
    "print(f\"\\nUNSTRUCTURED STRING SAMPLES: {len(string_batch)}\\n\")\n",
    "print(f\"JSON STRING SAMPLES: {len(string_json_batch)}\\n\")\n",
    "\n",
    "\n",
    "string_samples = smart_parse_batch(string_batch)\n",
    "\n",
    "for i, (msg, parsed) in enumerate(string_samples):\n",
    "    print(f\"Original: {msg}\\nParsed: {parsed}\\n\")\n",
    "\n",
    "\n",
    "json_samples = []\n",
    "leftovers = []\n",
    "\n",
    "for msg in string_json_batch:\n",
    "    try:\n",
    "        data = json.loads(msg)\n",
    "        json_samples.append((msg, data))\n",
    "    except Exception:\n",
    "        leftovers.append((msg, None))\n",
    "\n",
    "\n",
    "data_samples = string_samples + json_samples\n",
    "\n",
    "print(f\"\\nTOTAL SAMPLES: {len(data_samples)}\\nERROR SAMPLES: {len(leftovers)}\\n\")\n",
    "\n",
    "#################################################################################\n",
    "############################### Structure Analyzer ##############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "analyzer = StructuralAnalyzer(ignore_unparsed=False)\n",
    "\n",
    "hash_counts = Counter()\n",
    "unique_structures = {}\n",
    "\n",
    "for raw, parsed in data_samples:\n",
    "    result = analyzer.generate_fingerprint(raw, parsed)\n",
    "    signature_hash = result[\"hash\"]\n",
    "    hash_counts[signature_hash] += 1\n",
    "    unique_structures[signature_hash] = unique_structures.get(signature_hash, result)\n",
    "\n",
    "print(\n",
    "    f\"Detected {len(unique_structures)} unique schemas across {len(data_samples)} records.\\n\"\n",
    ")\n",
    "\n",
    "for h, count in sorted(hash_counts.items()):\n",
    "    print(f\"Schema {h} ({count} occurrences):\")\n",
    "    print(f\"  Layout: {unique_structures[h]['signature']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "############################# Structural Clustering #############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "def conjoin_signatures(registry_output: dict):\n",
    "    # 1. Extract the unique signatures and their keys\n",
    "    # registry_output is that \"all-encompassing\" JSON you shared\n",
    "    hashes = list(registry_output.keys())\n",
    "\n",
    "    # Create a \"sentence\" of keys for each hash\n",
    "    # e.g., \"order buyer location total items\"\n",
    "    signatures_as_text = [\n",
    "        \" \".join(registry_output[h][\"signature\"].keys()) for h in hashes\n",
    "    ]\n",
    "\n",
    "    # 2. Vectorize using character n-grams (resilient to typos)\n",
    "    # This turns 'locadtion' and 'location' into very similar vectors\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5))\n",
    "    matrix = vectorizer.fit_transform(signatures_as_text)\n",
    "\n",
    "    # 3. Cluster using HDBSCAN\n",
    "    # min_cluster_size=2 because we want to find groups of variants\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=2,\n",
    "        metric=\"euclidean\",\n",
    "        copy=True,\n",
    "    )\n",
    "    labels = clusterer.fit_predict(matrix.toarray())\n",
    "\n",
    "    # 4. Map Hashes to Cluster Groups\n",
    "    conjoined_map = {}\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        h = hashes[i]\n",
    "        conjoined_map[h] = {\n",
    "            \"cluster_id\": int(cluster_id),\n",
    "            \"keys\": list(registry_output[h][\"signature\"].keys()),\n",
    "            \"is_outlier\": cluster_id == -1,\n",
    "        }\n",
    "\n",
    "    sorted_dict = dict(sorted(conjoined_map.items()))\n",
    "    return sorted_dict\n",
    "\n",
    "#################################################################################\n",
    "############################## Semantic Clustering ##############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "def conjoin_signatures_semantic(\n",
    "    registry_output: dict, \n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\", \n",
    "    cache_dir: str = \"./.models\"\n",
    "):\n",
    "    hashes = list(registry_output.keys())\n",
    "    if not hashes: \n",
    "        return {}\n",
    "\n",
    "    signatures_as_text = []\n",
    "    for h in hashes:\n",
    "        h_dict = dict(registry_output[h][\"signature\"])\n",
    "        # remove the 'black hole' field that swallows everything\n",
    "        h_dict.pop(\"_unparsed\", None)\n",
    "        \n",
    "        # sort keys to ensure structural identity regardless of log order\n",
    "        sorted_keys = sorted(h_dict.keys())\n",
    "        \n",
    "        if not sorted_keys:\n",
    "            text_rep = \"schema:empty_blob\"\n",
    "        else:\n",
    "            # 'field:' prefix to define the role of the tokens\n",
    "            text_rep = \" \".join([f\"field:{k}\" for k in sorted_keys])\n",
    "            \n",
    "        signatures_as_text.append(text_rep)\n",
    "\n",
    "    model = SentenceTransformer(embedding_model, cache_folder=cache_dir)\n",
    "    embeddings = model.encode(signatures_as_text)\n",
    "    X = np.ascontiguousarray(embeddings, dtype=np.float64)\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=2,\n",
    "        min_samples=1,\n",
    "        metric=\"cosine\",\n",
    "        cluster_selection_epsilon=0.08, \n",
    "        cluster_selection_method=\"eom\",\n",
    "        allow_single_cluster=True,\n",
    "    )\n",
    "\n",
    "    labels = clusterer.fit_predict(X)\n",
    "\n",
    "    conjoined_map = {}\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        h = hashes[i]\n",
    "        # unique IDs to outliers so they don't group into one '-1' bucket\n",
    "        final_id = int(cluster_id) if cluster_id != -1 else (400 + i)\n",
    "        \n",
    "        conjoined_map[h] = {\n",
    "            \"cluster_id\": final_id,\n",
    "            \"keys\": list(registry_output[h][\"signature\"].keys()),\n",
    "            \"is_outlier\": cluster_id == -1,\n",
    "        }\n",
    "\n",
    "    return conjoined_map\n",
    "\n",
    "# run structure clustering\n",
    "structure_cluster_map = conjoin_signatures(analyzer.signature_map)\n",
    "structure_clusters = {}\n",
    "for k, v in structure_cluster_map.items():\n",
    "    cluster_id = v[\"cluster_id\"]\n",
    "    keys = v[\"keys\"]\n",
    "    is_outlier = bool(v[\"is_outlier\"])\n",
    "    structure_clusters[cluster_id] = structure_clusters.get(cluster_id, [])\n",
    "    structure_clusters[cluster_id].append(\n",
    "        {\"signature_hash\": k, \"fields\": keys, \"is_outlier\": is_outlier}\n",
    "    )\n",
    "print(f\"Structural Clusters: \\n{json.dumps(structure_clusters, indent=4)}\\n\")\n",
    "\n",
    "\n",
    "# run semantic clustering\n",
    "print(json.dumps(analyzer.signature_map, indent=4))\n",
    "semantic_cluster_map = conjoin_signatures_semantic(analyzer.signature_map)\n",
    "semantic_clusters = {}\n",
    "for k, v in semantic_cluster_map.items():\n",
    "    cluster_id = v[\"cluster_id\"]\n",
    "    keys = v[\"keys\"]\n",
    "    is_outlier = bool(v[\"is_outlier\"])\n",
    "    semantic_clusters[cluster_id] = semantic_clusters.get(cluster_id, [])\n",
    "    semantic_clusters[cluster_id].append(\n",
    "        {\"signature_hash\": k, \"fields\": keys, \"is_outlier\": is_outlier}\n",
    "    )\n",
    "print(f\"Semantic Clusters: \\n{json.dumps(semantic_clusters, indent=4)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb800c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [{'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable',\n",
       "   'parsed': {'order': '1001',\n",
       "    'buyer': 'John Davis',\n",
       "    'location': 'Columbus, OH',\n",
       "    'total': '$742.10',\n",
       "    'items': 'laptop, hdmi cable'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp',\n",
       "   'parsed': {'order': '1004',\n",
       "    'buyer': 'AMANDA SMITH',\n",
       "    'location': 'Seattle, WA',\n",
       "    'total': '$50.00',\n",
       "    'items': 'desk lamp'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard',\n",
       "   'parsed': {'order': '1006',\n",
       "    'total': '$89.99',\n",
       "    'location': 'Miami, FL',\n",
       "    'buyer': 'Elena Rossi',\n",
       "    'items': 'keyboard'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]',\n",
       "   'parsed': {'order': '1007',\n",
       "    'buyer': 'Chris P.',\n",
       "    'location': 'Denver, CO',\n",
       "    'total': '$12.00',\n",
       "    'items': 'stickers -- [DISCOUNT APPLIED]'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': \"Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \",\n",
       "   'parsed': {'order': '1008',\n",
       "    'buyer': \"O'Connor, S.\",\n",
       "    'location': 'Portland, OR',\n",
       "    'total': '$0.00',\n",
       "    'items': 'None'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable',\n",
       "   'parsed': {'order': '1011',\n",
       "    'buyer': 'John Davis',\n",
       "    'location': 'Columbus, OH',\n",
       "    'total': '$742.10',\n",
       "    'items': 'laptop, hdmi cable'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones',\n",
       "   'parsed': {'order': '1012',\n",
       "    'buyer': 'Sarah Liu',\n",
       "    'location': 'Austin, TX',\n",
       "    'total': '$156.55',\n",
       "    'items': 'headphones'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse',\n",
       "   'parsed': {'order': '1013',\n",
       "    'buyer': 'Mike Turner',\n",
       "    'location': 'Cleveland, OH',\n",
       "    'total': '$1299.99',\n",
       "    'items': 'gaming pc, mouse'}},\n",
       "  {'signature_hash': 'fd116cd512d5ecd2e59edf12fc258b32',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp',\n",
       "   'parsed': {'order': '1015',\n",
       "    'buyer': 'Chris Myers',\n",
       "    'location': 'Cincinnati, OH',\n",
       "    'total': '$512.00',\n",
       "    'items': 'monitor, desk lamp'}},\n",
       "  {'signature_hash': '50eb97a85647221ecc7f65f74d68d156',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable',\n",
       "   'parsed': {'order': '1005',\n",
       "    'buyer': 'Raj Patel',\n",
       "    'total': '1,200.50',\n",
       "    'items': 'monitor, stand, cable'}},\n",
       "  {'signature_hash': '50eb97a85647221ecc7f65f74d68d156',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,',\n",
       "   'parsed': {'order': '1016',\n",
       "    'buyer': 'Jake Myers',\n",
       "    'total': '$1,512.00',\n",
       "    'items': 'monitor'}},\n",
       "  {'signature_hash': '28d9f3b14d0e5516a186062212502d0c',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker',\n",
       "   'parsed': {'order': '1014',\n",
       "    'buyer': 'Rachel Kim',\n",
       "    'locadtion': 'Seattle, WA',\n",
       "    'total': '$89.50',\n",
       "    'items': 'coffee maker'}},\n",
       "  {'signature_hash': '461a895ef9c5046dd2cb5026b6a62de0',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes',\n",
       "   'parsed': {'order': '1017',\n",
       "    'buyer': 'Griffin Arora',\n",
       "    'location': 'Columbia, SC',\n",
       "    'total': '$512.00',\n",
       "    'items': 'monitor, desk lamp',\n",
       "    'discount': 'yes'}},\n",
       "  {'signature_hash': '7299d10d006e4a3fc143b9d77e79917e',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order=1018, Buyer=Jae Arora, Location=Dreher, FL, Total=$6.00, Items: chair, Discount: true, phone=123-456-789',\n",
       "   'parsed': {'order': '1018',\n",
       "    'buyer': 'Jae Arora',\n",
       "    'location': 'Dreher, FL',\n",
       "    'total': '$6.00',\n",
       "    'items': 'chair',\n",
       "    'discount': 'true',\n",
       "    'phone': '123-456-789'}},\n",
       "  {'signature_hash': '7299d10d006e4a3fc143b9d77e79917e',\n",
       "   'structure_cluster_id': 1,\n",
       "   'raw': 'Order=1019, Buyer=Jae Kao, Location=Atlanta, GA, Total=$12.00, Items: desk, Discount: False, phone=123-456-789',\n",
       "   'parsed': {'order': '1019',\n",
       "    'buyer': 'Jae Kao',\n",
       "    'location': 'Atlanta, GA',\n",
       "    'total': '$12.00',\n",
       "    'items': 'desk',\n",
       "    'discount': 'False',\n",
       "    'phone': '123-456-789'}}],\n",
       " 1: [{'signature_hash': '3baae1f59cac077e89e2f0b7d47a36cf',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '2026-01-30 14:22:01 INFO User login successful user_id=123',\n",
       "   'parsed': {'user_id': '123',\n",
       "    '_unparsed': '2026-01-30 14:22:01 INFO User login successful'}},\n",
       "  {'signature_hash': 'c2aebc126ff252eb9dae084d659d2cab',\n",
       "   'structure_cluster_id': 2,\n",
       "   'raw': 'level =INFO, user =Sam, id=1',\n",
       "   'parsed': {'level': 'INFO', 'user': 'Sam', 'id': '1'}},\n",
       "  {'signature_hash': '74db6b18a3e440d4dd054614492fa510',\n",
       "   'structure_cluster_id': 2,\n",
       "   'raw': 'timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true',\n",
       "   'parsed': {'timestamp': '2026-01-30T14:22:01Z',\n",
       "    'level': 'INFO',\n",
       "    'user': 'alice',\n",
       "    'action': 'login',\n",
       "    'success': 'true'}}],\n",
       " 406: [{'signature_hash': 'df87355cb94200d773396f5befa867d4',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '2026-01-30 14:22:01 INFO User login successful',\n",
       "   'parsed': {'_unparsed': '2026-01-30 14:22:01 INFO User login successful'}},\n",
       "  {'signature_hash': 'df87355cb94200d773396f5befa867d4',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"',\n",
       "   'parsed': {'_unparsed': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"'}},\n",
       "  {'signature_hash': 'df87355cb94200d773396f5befa867d4',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check',\n",
       "   'parsed': {'_unparsed': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check'}}],\n",
       " 409: [{'signature_hash': 'd0f1c5f0782666cd8324b4cf57a3159d',\n",
       "   'structure_cluster_id': 2,\n",
       "   'raw': 'level=INFO cpu_usage=1,234.56 memory=512MB',\n",
       "   'parsed': {'level': 'INFO', 'cpu_usage': '1,234.56', 'memory': '512MB'}}],\n",
       " 2: [{'signature_hash': 'a0713c61038426204fef9da85c91686f',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": \"usr_001\", \"name\": \"Alex Johnson\", \"role\": \"admin\", \"isActive\": true, \"createdAt\": \"2025-11-02T09:14:23Z\"}',\n",
       "   'parsed': {'id': 'usr_001',\n",
       "    'name': 'Alex Johnson',\n",
       "    'role': 'admin',\n",
       "    'isActive': True,\n",
       "    'createdAt': '2025-11-02T09:14:23Z'}},\n",
       "  {'signature_hash': '4286ed8928a42e13682233f00079d106',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": \"usr_002\", \"name\": \"Maria Lopez\", \"email\": \"maria.lopez@example.com\", \"role\": \"editor\", \"isActive\": null, \"createdAt\": \"2025-12-18T16:47:10Z\", \"lastLoginIp\": \"192.168.1.42\"}',\n",
       "   'parsed': {'id': 'usr_002',\n",
       "    'name': 'Maria Lopez',\n",
       "    'email': 'maria.lopez@example.com',\n",
       "    'role': 'editor',\n",
       "    'isActive': None,\n",
       "    'createdAt': '2025-12-18T16:47:10Z',\n",
       "    'lastLoginIp': '192.168.1.42'}},\n",
       "  {'signature_hash': 'd5d811706d95e2f6c1b713aa3fd1d9ab',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": \"usr_003\", \"email\": \"samir.patel@example.com\", \"role\": \"viewer\", \"isActive\": false, \"createdAt\": \"08/05/2024\"}',\n",
       "   'parsed': {'id': 'usr_003',\n",
       "    'email': 'samir.patel@example.com',\n",
       "    'role': 'viewer',\n",
       "    'isActive': False,\n",
       "    'createdAt': '08/05/2024'}},\n",
       "  {'signature_hash': '2bcfb738a2056c5ef5543917c9f3ac53',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": 4, \"name\": \"Chen Wei\", \"email\": \"chen.wei@example.com\", \"isActive\": true, \"createdAt\": null}',\n",
       "   'parsed': {'id': 4,\n",
       "    'name': 'Chen Wei',\n",
       "    'email': 'chen.wei@example.com',\n",
       "    'isActive': True,\n",
       "    'createdAt': None}},\n",
       "  {'signature_hash': 'd2d16f7c3698c6195ddaeb6205139150',\n",
       "   'structure_cluster_id': 0,\n",
       "   'raw': '{\"id\": \"usr_005\", \"name\": \"Broken Record\", \"email\": \"broken@example.com\"}',\n",
       "   'parsed': {'id': 'usr_005',\n",
       "    'name': 'Broken Record',\n",
       "    'email': 'broken@example.com'}}],\n",
       " 415: [{'signature_hash': 'e8b13b0b419896ad0f402c15a762e2c1',\n",
       "   'structure_cluster_id': 3,\n",
       "   'raw': '{\"level\":\"INFO\",\"service\":\"orders\",\"order_id\":1001,\"status\":\"created\"}',\n",
       "   'parsed': {'level': 'INFO',\n",
       "    'service': 'orders',\n",
       "    'order_id': 1001,\n",
       "    'status': 'created'}}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# semantic cluster map to raw string, rough parsed dict, structure cluster, structure signature hash\n",
    "cluster_dict = {}\n",
    "for cluster_id, records in semantic_clusters.items():\n",
    "    cluster_dict[cluster_id] = cluster_dict.get(cluster_id, [])\n",
    "    for rec in records:\n",
    "        signature_hash = rec[\"signature_hash\"]\n",
    "        analyzer_records = analyzer.signature_map[signature_hash][\"records\"]\n",
    "        for r in analyzer_records:\n",
    "            cluster_dict[cluster_id].append(\n",
    "                {\n",
    "                    \"signature_hash\": signature_hash,\n",
    "                    \"structure_cluster_id\": structure_cluster_map[signature_hash].get(\n",
    "                        \"cluster_id\"\n",
    "                    ),\n",
    "                    \"raw\": r[\"raw\"],\n",
    "                    \"parsed\": r[\"parsed\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "cluster_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70354b4f",
   "metadata": {},
   "source": [
    "## LLM Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f369b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm connection\n",
    "API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "# api_base_url = os.getenv(\"LsLM_BASE_URL\")\n",
    "api_base_url = \"https://openrouter.ai/api/v1\"\n",
    "# model = \"openai/google/gemma-3-27b-it\"\n",
    "MODEL = \"openai/gpt-oss-120b:exacto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cea7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncLLMClient(\n",
    "    MODEL,\n",
    "    api_base_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc455a6a",
   "metadata": {},
   "source": [
    "## LLM Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "760f05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "PYDANTIC_SYSTEM_PROMPT = \"\"\"You are a Data Architect. Your goal is to perform unsupervised schema inference on a sample of unstructured data.\n",
    "\n",
    "Generate a Pydantic `BaseModel` class that represents the \"Gold Standard\" foundation for this data pattern. \n",
    "\n",
    "Instructions:\n",
    "- Normalization: Suggest clean, snake_case keys for the identified fields.\n",
    "- If you see a string value for a field that follows a consistent structure (e.g., \"<city>, <state>\") then make sure that structure is accurately typed in the BaseModel.\n",
    "- Determine what fields should be required vs optional based on overall semantic meaning of the entity you are creating a BaseModel class for.\n",
    "\n",
    "Constraints:\n",
    "- Include a detailed description for each field using the `Field` class to explain what the field is and if there are any expected structural patterns (e.g., `state` should be two letters).\n",
    "- Create supplemental BaseModel classes where necessary to preserve semantic clarity.\n",
    "- Do NOT include any regex.\n",
    "- You MUST wrap your code in a python block with the following start marking \"```python\" and end marking \"```\".\n",
    "- If a field appears in some rows but not others, mark it as `Optional`.\n",
    "- You are only allowed to use the following imports: \"from typing import List, Dict, Optional; from pydantic import BaseModel, Field\".\n",
    "- Return ONLY the Pydantic class definitions (you are allowed to generate multiple as long as they are logically linked).\n",
    "\"\"\"\n",
    "\n",
    "PYDANTIC_USER_PROMPT = \"\"\"Please analyze the following representative samples of a new data pattern and generate the Pydantic 'Foundation' model.\n",
    "\n",
    "### Data Samples:\n",
    "{samples}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c08fc",
   "metadata": {},
   "source": [
    "## Generate Pydantic BaseModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "303d5e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"code_string\": \"```python\\nfrom typing import List, Optional\\nfrom pydantic import BaseModel, Field\\n\\nclass Location(BaseModel):\\n    city: str = Field(..., description=\\\"City name extracted from the location field.\\\")\\n    state: str = Field(..., description=\\\"Two‑letter US state abbreviation, e.g., 'OH'.\\\")\\n\\nclass Order(BaseModel):\\n    order_id: int = Field(..., description=\\\"Numeric identifier of the order, extracted from the leading 'Order' token.\\\")\\n    buyer_name: str = Field(..., description=\\\"Full name of the buyer as it appears after the 'Buyer=' token.\\\")\\n    location: Optional[Location] = Field(\\n        None, description=\\\"Geographic location of the buyer; may be missing in some records.\\\"\\n    )\\n    total_amount: float = Field(\\n        ..., description=\\\"Monetary total for the order, parsed as a float. Currency symbols and commas are ignored.\\\"\\n    )\\n    items: List[str] = Field(\\n        default_factory=list,\\n        description=\\\"List of item names purchased, split by commas. Empty list if no items are listed.\\\"\\n    )\\n    discount_applied: Optional[bool] = Field(\\n        None, description=\\\"Indicates whether a discount was applied. Accepts true/false, yes/no, or may be absent.\\\"\\n    )\\n    phone: Optional[str] = Field(\\n        None, description=\\\"Contact phone number associated with the order, if present.\\\"\\n    )\\n```\",\n",
      "  \"entrypoint_class_name\": \"Order\"\n",
      "}\n",
      "Successfully wrote code to order_base_model.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ModelResponseStructure(BaseModel):\n",
    "    code_string: str = Field(..., description=\"generated python code\")\n",
    "    entrypoint_class_name: str = Field(\n",
    "        ..., description=\"name of entrypoint base model class in the code generated\"\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extracts the Python code block from a string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted source code or an empty string if not found.\n",
    "    \"\"\"\n",
    "    block_pattern = r\"```(?:python)?\\s*(.*?)\\s*```\"\n",
    "    match = re.search(block_pattern, text, re.DOTALL)\n",
    "\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "for c_id, records in cluster_dict.items():\n",
    "    string_li = [r[\"raw\"] for r in records]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": PYDANTIC_SYSTEM_PROMPT},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": PYDANTIC_USER_PROMPT.format(\n",
    "                samples=string_li,\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "        response_format=ModelResponseStructure,\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    resp: ModelResponseStructure = ModelResponseStructure(**json.loads(resp))\n",
    "\n",
    "    if not resp.code_string.startswith(\"```python\"):\n",
    "        resp.code_string = f\"```python\\n{resp.code_string}\\n```\"\n",
    "\n",
    "    code = extract_python_code(resp.code_string)\n",
    "\n",
    "    namespace = {}\n",
    "    exec(code, globals(), namespace)\n",
    "\n",
    "    # access the function from the namespace dictionary\n",
    "    cls = namespace.get(resp.entrypoint_class_name)\n",
    "    cls.model_rebuild(_types_namespace=namespace)\n",
    "    schema = cls.model_json_schema()\n",
    "\n",
    "    fname = f\"{resp.entrypoint_class_name.lower()}_base_model.py\"\n",
    "    code = code.encode(\"ascii\", errors=\"ignore\").decode(\"ascii\")\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(code)\n",
    "        print(f\"\\nSuccessfully wrote code to {fname}\")\n",
    "\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a80c7b",
   "metadata": {},
   "source": [
    "## Langgraph Robustness and Stategraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab909348",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECT_PROMPT = \"\"\"You are a Lead Data Architect.\n",
    "Define a simple Pydantic v2 `BaseModel` that represents the \"Gold Standard\" foundation for the data pattern found in the input samples.\n",
    "\n",
    "INPUT SAMPLES (Multiple variations):\n",
    "{samples}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Normalization: Suggest clean, snake_case keys for the identified fields.\n",
    "2. Optionality: If a field is missing in ANY of the samples, you MUST wrap it in Optional[...].\n",
    "3. Determine what fields should be required vs optional based on overall semantic meaning of the entity you are creating a BaseModel class for.\n",
    "4. Include a detailed description for each field using the `Field` class to explain what the field is and if there are any expected structural patterns (e.g., `state` should be two letters).\n",
    "5. Do NOT include any regex.\n",
    "6. You MUST wrap your code in a python block with the following start marking \"```python\" and end marking \"```\".\n",
    "7. Create supplemental BaseModel classes where necessary to preserve semantic clarity.\n",
    "8. You are ONLY allowed to use the following imports: \"from typing import List, Dict, Optional; from pydantic import BaseModel, Field\".\n",
    "9. Keep primary keys as type string.\n",
    "10. Infer best data type from string value (e.g., money should be a float, \"true/false\" or \"yes/no\" fields should be a boolean, and fields that represent multiple entities should use a representative aggregate data structure type)\n",
    "11. NEVER set potentially boolean fields as optional. Instead, when not explicitly declared, infer as to what the default value ought based on the semantic meaning of the field and how it appears in the samples that do provide it.\n",
    "12. Perform semantic merging: Identify fields across structural variants that share the same intent and conjoin them under a single, definitive schema key to avoid redundancy (e.g., \"location\" vs \"city\", \"state\", \"zip code\")\n",
    "13. Avoid information loss when it comes to key:value pairs in the sample data.\n",
    "\n",
    "Return ONLY the Python code for the class. Include necessary imports (from pydantic import BaseModel, Field, etc.).\n",
    "\"\"\"\n",
    "\n",
    "CODER_PROMPT = \"\"\"You are a Senior Data Engineer.\n",
    "Your task is to write a concise but effective transformation function `transform_to_models(parsed_dict: list[dict]) -> list[dict]` that maps roughly parsed dictionaries into the provided pydantic v2 target schema base model definition.\n",
    "\n",
    "TARGET SCHEMA (Python Pydantic v2 BaseModel):\n",
    "{schema}\n",
    "\n",
    "SOURCE SAMPLES:\n",
    "{samples}\n",
    "\n",
    "Logic Requirements:\n",
    "1. Use a 'coalesce' approach: for each target field, check all possible source keys from the input dictionary samples.\n",
    "2. Use parsed_dict.get() for optional fields.\n",
    "3. Infer best data type from string (e.g., \"$120.00\" should be a float, and \"true\" should be a boolean). \n",
    "4. ALL python code must be encapsulated by the `transform_to_models()` function -- if it's not in that function it will not be run.\n",
    "\n",
    "Return ONLY the Python code for the function `transform_to_models`. Do not include the Pydantic class in your response.\n",
    "\"\"\"\n",
    "\n",
    "CODE_EXECUTION = \"\"\"\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import *\n",
    "import json, re\n",
    "\n",
    "{schema}\n",
    "\n",
    "{parser_code}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "80a80d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import operator\n",
    "import traceback\n",
    "from typing import Annotated, Any, Dict, List, Literal, Optional, TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from dq_swirl.utils.log_utils import get_custom_logger\n",
    "\n",
    "logger = get_custom_logger()\n",
    "\n",
    "\n",
    "class ModelResponseStructure(BaseModel):\n",
    "    code_string: str = Field(..., description=\"generated python code\")\n",
    "    entrypoint_class_name: str = Field(\n",
    "        ...,\n",
    "        description=\"name of entrypoint base model class in the code generated\",\n",
    "    )\n",
    "\n",
    "\n",
    "class MultiAgentState(TypedDict):\n",
    "    semantic_id: str\n",
    "    structure_cluster_id: str\n",
    "    data_pairs_all: List[Dict[str, Any]]\n",
    "    data_pairs_structure: List[Dict[str, Any]]\n",
    "    # Reducers: 'new' replaces 'old' for strings/objects, but we sum 'attempts'\n",
    "    gold_schema: Annotated[Optional[ModelResponseStructure], lambda old, new: new]\n",
    "    parser_code: Annotated[Optional[str], lambda old, new: new]\n",
    "    feedback: Annotated[Optional[str], lambda old, new: new]\n",
    "    error_type: Annotated[\n",
    "        Optional[Literal[\"SCHEMA_ISSUE\", \"CODE_ISSUE\"]], lambda old, new: new\n",
    "    ]\n",
    "    attempts: Annotated[int, operator.add]  # Use addition to\n",
    "\n",
    "\n",
    "async def architect_node(state: MultiAgentState):\n",
    "    if state.get(\"gold_schema\") and state.get(\"error_type\") != \"SCHEMA_ISSUE\":\n",
    "        return {\"attempts\": 0}\n",
    "\n",
    "    logger.info(f\"[Architect] Defining Semantic Goal: {state['attempts']}\")\n",
    "    # Send a variety of samples so the architect sees all potential fields\n",
    "    samples = json.dumps([p[\"parsed\"] for p in state[\"data_pairs_all\"][:100]], indent=2)\n",
    "\n",
    "    logger.debug(samples)\n",
    "\n",
    "    prompt = ARCHITECT_PROMPT.format(\n",
    "        samples=samples,\n",
    "    )\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        api_key_override=API_KEY,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "        response_format=ModelResponseStructure,\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    resp = ModelResponseStructure(**json.loads(resp))\n",
    "    resp.code_string = extract_python_code(resp.code_string)\n",
    "    print(resp.code_string)\n",
    "\n",
    "    return {\n",
    "        \"gold_schema\": resp,\n",
    "        \"attempts\": 1,\n",
    "        \"feedback\": None,\n",
    "        \"error_type\": None,  # CLEAR the error type\n",
    "    }\n",
    "\n",
    "\n",
    "async def schema_tester_node(state: MultiAgentState):\n",
    "    logger.info(f\"[Scehma Tester] Validating Functional BaseModel: {state['attempts']}\")\n",
    "    python_base_model_str = state[\"gold_schema\"].code_string\n",
    "\n",
    "    env = {}\n",
    "    try:\n",
    "        exec(python_base_model_str, globals(), env)\n",
    "        cls_name = state[\"gold_schema\"].entrypoint_class_name\n",
    "        model = env[cls_name]\n",
    "\n",
    "        model.model_rebuild(_types_namespace=env)\n",
    "        schema = model.model_json_schema()\n",
    "\n",
    "        return {\"feedback\": \"SUCCESS\"}\n",
    "    except Exception as e:\n",
    "        err_msg = traceback.format_exc()\n",
    "        logger.exception(e)\n",
    "        return {\"feedback\": err_msg, \"error_type\": \"SCHEMA_ISSUE\"}\n",
    "\n",
    "\n",
    "async def coder_node(state: MultiAgentState):\n",
    "    logger.info(f\"[Coder] Parser for Gold Schema: {state['attempts']}\")\n",
    "    samples = json.dumps(\n",
    "        [rec[\"parsed\"] for rec in state[\"data_pairs_structure\"]], indent=2\n",
    "    )\n",
    "\n",
    "    prompt = CODER_PROMPT.format(\n",
    "        schema=state[\"gold_schema\"].code_string, samples=samples\n",
    "    )\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        api_key_override=API_KEY,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    code = extract_python_code(resp)\n",
    "\n",
    "    return {\"parser_code\": code, \"attempts\": 1, \"feedback\": None, \"error_type\": None}\n",
    "\n",
    "\n",
    "async def code_tester_node(state: MultiAgentState):\n",
    "    logger.info(f\"[Code Tester] Stress-testing parser: {state['attempts']}\")\n",
    "    full_code = CODE_EXECUTION.format(\n",
    "        schema=state[\"gold_schema\"].code_string, parser_code=state[\"parser_code\"]\n",
    "    )\n",
    "\n",
    "    env = {}\n",
    "    try:\n",
    "        cls_name = state[\"gold_schema\"].entrypoint_class_name\n",
    "        exec(full_code, globals(), env)\n",
    "        func = env[\"transform_to_models\"]\n",
    "        model = env[cls_name]\n",
    "        model.model_rebuild(_types_namespace=env)\n",
    "\n",
    "        input_data = [pair[\"parsed\"] for pair in state[\"data_pairs_structure\"]]\n",
    "        mapped_batch = func(input_data)\n",
    "        for mapped_dict in mapped_batch:\n",
    "            model.model_validate(mapped_dict)\n",
    "            print(f\"Input: {mapped_dict} -- PASSED\")\n",
    "        return {\"feedback\": \"SUCCESS\"}\n",
    "    except Exception as e:\n",
    "        err_msg = traceback.format_exc()\n",
    "        try:\n",
    "            print(f\"Input: {mapped_dict} -- FAILED\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {\"feedback\": err_msg, \"error_type\": \"CODE_ISSUE\"}\n",
    "\n",
    "\n",
    "async def exporter_node(state: MultiAgentState):\n",
    "    base_model_name = state[\"gold_schema\"].entrypoint_class_name\n",
    "    filename = f\"parser_{base_model_name}_struct_{state['structure_cluster_id']}.py\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(f\"{state['gold_schema'].code_string}\\n\\n{state['parser_code']}\")\n",
    "    print(f\"--> Exported: {filename}\")\n",
    "    return {\"feedback\": \"DONE\"}\n",
    "\n",
    "\n",
    "def schema_router(state: MultiAgentState):\n",
    "    \"\"\"Determines if we move to Coder or retry the Architect.\"\"\"\n",
    "    feedback = state.get(\"feedback\")\n",
    "    attempts = state.get(\"attempts\", 0)\n",
    "\n",
    "    if feedback == \"SUCCESS\":\n",
    "        return \"coder\"\n",
    "\n",
    "    # if failed too many times, just stop the process\n",
    "    if attempts >= 3:\n",
    "        logger.error(f\"Schema failed after {attempts} attempts. Aborting.\")\n",
    "        return \"end\"\n",
    "\n",
    "    return \"architect\"\n",
    "\n",
    "\n",
    "def code_router(state: MultiAgentState):\n",
    "    \"\"\"Determines if we export or retry Coder/Architect.\"\"\"\n",
    "    feedback = state.get(\"feedback\")\n",
    "    error_type = state.get(\"error_type\")\n",
    "    attempts = state.get(\"attempts\", 0)\n",
    "\n",
    "    if feedback == \"SUCCESS\":\n",
    "        return \"exporter\"\n",
    "\n",
    "    if attempts >= 6:\n",
    "        return \"end\"\n",
    "\n",
    "    # Specific routing based on where the failure happened\n",
    "    if error_type == \"SCHEMA_ISSUE\":\n",
    "        return \"architect\"\n",
    "\n",
    "    # Default to retrying the coder for CODE_ISSUE or unknown errors\n",
    "    return \"coder\"\n",
    "\n",
    "\n",
    "## Define Graph\n",
    "workflow = StateGraph(MultiAgentState)\n",
    "workflow.add_node(\"architect\", architect_node)\n",
    "workflow.add_node(\"schema_tester\", schema_tester_node)\n",
    "workflow.add_node(\"coder\", coder_node)\n",
    "workflow.add_node(\"code_tester\", code_tester_node)\n",
    "workflow.add_node(\"exporter\", exporter_node)\n",
    "\n",
    "workflow.add_edge(START, \"architect\")\n",
    "workflow.add_edge(\"architect\", \"schema_tester\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"schema_tester\",\n",
    "    schema_router,\n",
    "    {\"architect\": \"architect\", \"coder\": \"coder\", \"end\": END},\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"coder\", \"code_tester\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"code_tester\",\n",
    "    code_router,\n",
    "    {\"architect\": \"architect\", \"coder\": \"coder\", \"exporter\": \"exporter\", \"end\": END},\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"exporter\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d34e14de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:44:46\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:44\u001b[0m [Architect] Defining Semantic Goal: 0\n",
      "\u001b[38;5;245m2026-02-02 12:44:46\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[34;20mDEBUG\u001b[0m] \u001b[38;5;245m2336556215.py:48\u001b[0m [\n",
      "  {\n",
      "    \"order\": \"1001\",\n",
      "    \"buyer\": \"John Davis\",\n",
      "    \"location\": \"Columbus, OH\",\n",
      "    \"total\": \"$742.10\",\n",
      "    \"items\": \"laptop, hdmi cable\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1004\",\n",
      "    \"buyer\": \"AMANDA SMITH\",\n",
      "    \"location\": \"Seattle, WA\",\n",
      "    \"total\": \"$50.00\",\n",
      "    \"items\": \"desk lamp\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1006\",\n",
      "    \"total\": \"$89.99\",\n",
      "    \"location\": \"Miami, FL\",\n",
      "    \"buyer\": \"Elena Rossi\",\n",
      "    \"items\": \"keyboard\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1007\",\n",
      "    \"buyer\": \"Chris P.\",\n",
      "    \"location\": \"Denver, CO\",\n",
      "    \"total\": \"$12.00\",\n",
      "    \"items\": \"stickers -- [DISCOUNT APPLIED]\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1008\",\n",
      "    \"buyer\": \"O'Connor, S.\",\n",
      "    \"location\": \"Portland, OR\",\n",
      "    \"total\": \"$0.00\",\n",
      "    \"items\": \"None\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1011\",\n",
      "    \"buyer\": \"John Davis\",\n",
      "    \"location\": \"Columbus, OH\",\n",
      "    \"total\": \"$742.10\",\n",
      "    \"items\": \"laptop, hdmi cable\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1012\",\n",
      "    \"buyer\": \"Sarah Liu\",\n",
      "    \"location\": \"Austin, TX\",\n",
      "    \"total\": \"$156.55\",\n",
      "    \"items\": \"headphones\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1013\",\n",
      "    \"buyer\": \"Mike Turner\",\n",
      "    \"location\": \"Cleveland, OH\",\n",
      "    \"total\": \"$1299.99\",\n",
      "    \"items\": \"gaming pc, mouse\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1015\",\n",
      "    \"buyer\": \"Chris Myers\",\n",
      "    \"location\": \"Cincinnati, OH\",\n",
      "    \"total\": \"$512.00\",\n",
      "    \"items\": \"monitor, desk lamp\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1005\",\n",
      "    \"buyer\": \"Raj Patel\",\n",
      "    \"total\": \"1,200.50\",\n",
      "    \"items\": \"monitor, stand, cable\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1016\",\n",
      "    \"buyer\": \"Jake Myers\",\n",
      "    \"total\": \"$1,512.00\",\n",
      "    \"items\": \"monitor\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1014\",\n",
      "    \"buyer\": \"Rachel Kim\",\n",
      "    \"locadtion\": \"Seattle, WA\",\n",
      "    \"total\": \"$89.50\",\n",
      "    \"items\": \"coffee maker\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1017\",\n",
      "    \"buyer\": \"Griffin Arora\",\n",
      "    \"location\": \"Columbia, SC\",\n",
      "    \"total\": \"$512.00\",\n",
      "    \"items\": \"monitor, desk lamp\",\n",
      "    \"discount\": \"yes\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1018\",\n",
      "    \"buyer\": \"Jae Arora\",\n",
      "    \"location\": \"Dreher, FL\",\n",
      "    \"total\": \"$6.00\",\n",
      "    \"items\": \"chair\",\n",
      "    \"discount\": \"true\",\n",
      "    \"phone\": \"123-456-789\"\n",
      "  },\n",
      "  {\n",
      "    \"order\": \"1019\",\n",
      "    \"buyer\": \"Jae Kao\",\n",
      "    \"location\": \"Atlanta, GA\",\n",
      "    \"total\": \"$12.00\",\n",
      "    \"items\": \"desk\",\n",
      "    \"discount\": \"False\",\n",
      "    \"phone\": \"123-456-789\"\n",
      "  }\n",
      "]\n",
      "\u001b[38;5;245m2026-02-02 12:44:58\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:85\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 12:44:58\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List, Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class Order(BaseModel):\n",
      "    order: str = Field(..., description=\"Unique order identifier as a string.\")\n",
      "    buyer: str = Field(..., description=\"Name of the buyer.\")\n",
      "    location: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Full location in 'City, ST' format. Optional because some records lack this field.\"\n",
      "    )\n",
      "    total: float = Field(\n",
      "        ..., \n",
      "        description=\"Total amount of the order as a float. Currency symbols and commas are ignored during parsing.\"\n",
      "    )\n",
      "    items: List[str] = Field(\n",
      "        ..., \n",
      "        description=\"List of items purchased. In the source data items are comma‑separated strings.\"\n",
      "    )\n",
      "    discount: bool = Field(\n",
      "        False,\n",
      "        description=\"Flag indicating whether a discount was applied. Defaults to False when the field is absent.\"\n",
      "    )\n",
      "    phone: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Contact phone number for the buyer, if provided.\"\n",
      "    )\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries that\n",
      "    conform to the ``Order`` Pydantic model schema.\n",
      "\n",
      "    The function follows a *coalesce* strategy – it looks for every possible source\n",
      "    key that could map to a target field, normalises the value and returns the\n",
      "    cleaned representation.\n",
      "    \"\"\"\n",
      "    from typing import Any, List\n",
      "\n",
      "    def _to_float(value: Any) -> float:\n",
      "        \"\"\"Convert a monetary string (e.g., '$1,200.50') or numeric to float.\"\"\"\n",
      "        if isinstance(value, (int, float)):\n",
      "            return float(value)\n",
      "        if not isinstance(value, str):\n",
      "            raise ValueError(f\"Cannot convert {value!r} to float\")\n",
      "        # strip currency symbols, commas and surrounding whitespace\n",
      "        cleaned = value.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
      "        try:\n",
      "            return float(cleaned)\n",
      "        except ValueError as exc:\n",
      "            raise ValueError(f\"Unable to parse total amount from {value!r}\") from exc\n",
      "\n",
      "    def _to_bool(value: Any) -> bool:\n",
      "        \"\"\"Interpret common truthy / falsy strings as booleans.\"\"\"\n",
      "        if isinstance(value, bool):\n",
      "            return value\n",
      "        if isinstance(value, (int, float)):\n",
      "            return bool(value)\n",
      "        if not isinstance(value, str):\n",
      "            return False\n",
      "        lowered = value.strip().lower()\n",
      "        return lowered in {\"yes\", \"true\", \"y\", \"1\"}\n",
      "\n",
      "    def _parse_items(value: Any) -> List[str]:\n",
      "        \"\"\"\n",
      "        Turn the raw ``items`` field into a list of clean item strings.\n",
      "        - ``None`` or empty strings become an empty list.\n",
      "        - If the string contains a ``--`` separator (e.g. \"stickers -- [DISCOUNT APPLIED]\")\n",
      "          only the left‑hand side is considered.\n",
      "        - Items are split on commas, stripped of whitespace and empty entries are dropped.\n",
      "        \"\"\"\n",
      "        if not isinstance(value, str):\n",
      "            return []\n",
      "        # Remove any trailing notes after '--'\n",
      "        main_part = value.split(\"--\")[0]\n",
      "        # Treat the literal word \"None\" as no items\n",
      "        if main_part.strip().lower() == \"none\":\n",
      "            return []\n",
      "        # Split on commas\n",
      "        parts = [p.strip() for p in main_part.split(\",\")]\n",
      "        # Filter out empty strings\n",
      "        return [p for p in parts if p]\n",
      "\n",
      "    transformed: List[dict] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        # --- order (required) -------------------------------------------------\n",
      "        order = src.get(\"order\")\n",
      "        if order is None:\n",
      "            raise ValueError(\"Missing required field 'order'\")\n",
      "\n",
      "        # --- buyer (required) -------------------------------------------------\n",
      "        buyer = src.get(\"buyer\")\n",
      "        if buyer is None:\n",
      "            raise ValueError(\"Missing required field 'buyer'\")\n",
      "\n",
      "        # --- location (optional, coalesce misspelling) -------------------------\n",
      "        location = src.get(\"location\") or src.get(\"locadtion\")  # typo handling\n",
      "\n",
      "        # --- total (required, convert to float) -------------------------------\n",
      "        raw_total = src.get(\"total\")\n",
      "        if raw_total is None:\n",
      "            raise ValueError(f\"Missing required field 'total' for order {order}\")\n",
      "        total = _to_float(raw_total)\n",
      "\n",
      "        # --- items (required, list of strings) -------------------------------\n",
      "        raw_items = src.get(\"items\")\n",
      "        if raw_items is None:\n",
      "            raise ValueError(f\"Missing required field 'items' for order {order}\")\n",
      "        items = _parse_items(raw_items)\n",
      "\n",
      "        # --- discount (optional, default False) -------------------------------\n",
      "        # Prefer explicit field; otherwise infer from items string.\n",
      "        raw_discount = src.get(\"discount\")\n",
      "        if raw_discount is not None:\n",
      "            discount = _to_bool(raw_discount)\n",
      "        else:\n",
      "            # Heuristic: presence of \"[DISCOUNT APPLIED]\" in the items string\n",
      "            discount = \"[discount applied]\" in str(raw_items).lower()\n",
      "\n",
      "        # --- phone (optional) -------------------------------------------------\n",
      "        phone = src.get(\"phone\")\n",
      "\n",
      "        transformed.append(\n",
      "            {\n",
      "                \"order\": order,\n",
      "                \"buyer\": buyer,\n",
      "                \"location\": location,\n",
      "                \"total\": total,\n",
      "                \"items\": items,\n",
      "                \"discount\": discount,\n",
      "                \"phone\": phone,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    return transformed\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:45:27\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 2\n",
      "\u001b[38;5;245m2026-02-02 12:45:28\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m1738028846.py:26\u001b[0m --- Finished Cluster 1 ---\n",
      "\u001b[38;5;245m2026-02-02 12:45:28\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:44\u001b[0m [Architect] Defining Semantic Goal: 0\n",
      "\u001b[38;5;245m2026-02-02 12:45:28\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[34;20mDEBUG\u001b[0m] \u001b[38;5;245m2336556215.py:48\u001b[0m [\n",
      "  {\n",
      "    \"user_id\": \"123\",\n",
      "    \"_unparsed\": \"2026-01-30 14:22:01 INFO User login successful\"\n",
      "  },\n",
      "  {\n",
      "    \"level\": \"INFO\",\n",
      "    \"user\": \"Sam\",\n",
      "    \"id\": \"1\"\n",
      "  },\n",
      "  {\n",
      "    \"timestamp\": \"2026-01-30T14:22:01Z\",\n",
      "    \"level\": \"INFO\",\n",
      "    \"user\": \"alice\",\n",
      "    \"action\": \"login\",\n",
      "    \"success\": \"true\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'order': '1001', 'buyer': 'John Davis', 'location': 'Columbus, OH', 'total': 742.1, 'items': ['laptop', 'hdmi cable'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1004', 'buyer': 'AMANDA SMITH', 'location': 'Seattle, WA', 'total': 50.0, 'items': ['desk lamp'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1006', 'buyer': 'Elena Rossi', 'location': 'Miami, FL', 'total': 89.99, 'items': ['keyboard'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1007', 'buyer': 'Chris P.', 'location': 'Denver, CO', 'total': 12.0, 'items': ['stickers'], 'discount': True, 'phone': None} -- PASSED\n",
      "Input: {'order': '1008', 'buyer': \"O'Connor, S.\", 'location': 'Portland, OR', 'total': 0.0, 'items': [], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1011', 'buyer': 'John Davis', 'location': 'Columbus, OH', 'total': 742.1, 'items': ['laptop', 'hdmi cable'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1012', 'buyer': 'Sarah Liu', 'location': 'Austin, TX', 'total': 156.55, 'items': ['headphones'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1013', 'buyer': 'Mike Turner', 'location': 'Cleveland, OH', 'total': 1299.99, 'items': ['gaming pc', 'mouse'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1015', 'buyer': 'Chris Myers', 'location': 'Cincinnati, OH', 'total': 512.0, 'items': ['monitor', 'desk lamp'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1005', 'buyer': 'Raj Patel', 'location': None, 'total': 1200.5, 'items': ['monitor', 'stand', 'cable'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1016', 'buyer': 'Jake Myers', 'location': None, 'total': 1512.0, 'items': ['monitor'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1014', 'buyer': 'Rachel Kim', 'location': 'Seattle, WA', 'total': 89.5, 'items': ['coffee maker'], 'discount': False, 'phone': None} -- PASSED\n",
      "Input: {'order': '1017', 'buyer': 'Griffin Arora', 'location': 'Columbia, SC', 'total': 512.0, 'items': ['monitor', 'desk lamp'], 'discount': True, 'phone': None} -- PASSED\n",
      "Input: {'order': '1018', 'buyer': 'Jae Arora', 'location': 'Dreher, FL', 'total': 6.0, 'items': ['chair'], 'discount': True, 'phone': '123-456-789'} -- PASSED\n",
      "Input: {'order': '1019', 'buyer': 'Jae Kao', 'location': 'Atlanta, GA', 'total': 12.0, 'items': ['desk'], 'discount': False, 'phone': '123-456-789'} -- PASSED\n",
      "--> Exported: parser_Order_struct_1.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:45:37\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:85\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 12:45:37\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class LogEntry(BaseModel):\n",
      "    \"\"\"Gold‑standard representation of a generic log / audit record.\n",
      "\n",
      "    The model normalises disparate source keys into a consistent, snake_case\n",
      "    schema while preserving all semantic information present in the example\n",
      "    payloads.\n",
      "    \"\"\"\n",
      "\n",
      "    user_id: Optional[str] = Field(\n",
      "        None,\n",
      "        description=(\n",
      "            \"Identifier of the user associated with the event. \"\n",
      "            \"May appear in source data as `user_id` or `id`.\"\n",
      "        ),\n",
      "    )\n",
      "    user: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Human‑readable username or account name.\",\n",
      "    )\n",
      "    level: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Log level such as `INFO`, `WARN`, `ERROR`, etc.\",\n",
      "    )\n",
      "    timestamp: Optional[str] = Field(\n",
      "        None,\n",
      "        description=(\n",
      "            \"ISO‑8601 formatted timestamp of the event, e.g. ``2026-01-30T14:22:01Z``.\"\n",
      "        ),\n",
      "    )\n",
      "    action: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Action performed by the user, e.g. `login`, `logout`, etc.\",\n",
      "    )\n",
      "    success: bool = Field(\n",
      "        False,\n",
      "        description=(\n",
      "            \"Flag indicating whether the recorded action succeeded. \"\n",
      "            \"Defaults to ``False`` when the field is absent in the source data.\"\n",
      "        ),\n",
      "    )\n",
      "    unparsed: Optional[str] = Field(\n",
      "        None,\n",
      "        description=(\n",
      "            \"Raw log line retained when the structured parsing could not extract \"\n",
      "            \"standard fields.\"\n",
      "        ),\n",
      "    )\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * coalesces multiple possible source keys for each target field,\n",
      "    * extracts values with ``dict.get``,\n",
      "    * infers a more appropriate Python type from string values\n",
      "      (e.g. \"$12.3\" → float, \"true\"/\"false\" → bool),\n",
      "    * supplies a default ``False`` for the ``success`` flag when absent.\n",
      "    \"\"\"\n",
      "    # Mapping of target field → possible source keys (ordered by priority)\n",
      "    field_map = {\n",
      "        \"user_id\": [\"user_id\", \"id\"],\n",
      "        \"user\": [\"user\", \"username\", \"name\"],\n",
      "        \"level\": [\"level\", \"log_level\", \"severity\"],\n",
      "        \"timestamp\": [\"timestamp\", \"time\", \"date\", \"datetime\"],\n",
      "        \"action\": [\"action\", \"event\", \"activity\"],\n",
      "        \"success\": [\"success\", \"status\", \"result\", \"outcome\"],\n",
      "        \"unparsed\": [\"_unparsed\", \"unparsed\", \"raw\"],\n",
      "    }\n",
      "\n",
      "    def _coerce(val):\n",
      "        \"\"\"Best‑effort conversion of a string to int, float or bool.\"\"\"\n",
      "        if isinstance(val, str):\n",
      "            s = val.strip()\n",
      "            # Boolean strings\n",
      "            if s.lower() in (\"true\", \"false\"):\n",
      "                return s.lower() == \"true\"\n",
      "            # Remove common numeric noise (currency symbols, commas)\n",
      "            cleaned = s.replace(\"$\", \"\").replace(\",\", \"\")\n",
      "            # Integer conversion\n",
      "            try:\n",
      "                return int(cleaned)\n",
      "            except ValueError:\n",
      "                pass\n",
      "            # Float conversion\n",
      "            try:\n",
      "                return float(cleaned)\n",
      "            except ValueError:\n",
      "                pass\n",
      "        return val\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        out: dict = {}\n",
      "        for target, candidates in field_map.items():\n",
      "            value = None\n",
      "            for key in candidates:\n",
      "                if key in src:\n",
      "                    value = src.get(key)\n",
      "                    break\n",
      "\n",
      "            if value is not None:\n",
      "                # Coerce to a more appropriate type\n",
      "                value = _coerce(value)\n",
      "\n",
      "                # ``success`` must be a bool; ensure correct type\n",
      "                if target == \"success\":\n",
      "                    out[\"success\"] = bool(value) if not isinstance(value, bool) else value\n",
      "                else:\n",
      "                    out[target] = value\n",
      "            else:\n",
      "                # Default handling when the field is missing\n",
      "                if target == \"success\":\n",
      "                    out[\"success\"] = False\n",
      "\n",
      "        # Guarantee the ``success`` key exists (covers empty source dicts)\n",
      "        out.setdefault(\"success\", False)\n",
      "        transformed.append(out)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:45:51\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 2\n",
      "\u001b[38;5;245m2026-02-02 12:45:51\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 123, 'success': False, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- FAILED\n",
      "```python\n",
      "import re\n",
      "from typing import Any, List, Dict, Optional\n",
      "\n",
      "def transform_to_models(parsed_dict: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"\n",
      "    Convert a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * coalesces multiple possible source keys for each target field,\n",
      "    * infers simple data types from string values (bool, float),\n",
      "    * supplies defaults (``success`` → ``False``) when a field is missing.\n",
      "    \"\"\"\n",
      "    # Mapping of target field → possible source keys (ordered by priority)\n",
      "    field_map: Dict[str, List[str]] = {\n",
      "        \"user_id\": [\"user_id\", \"id\"],\n",
      "        \"user\": [\"user\", \"username\", \"account_name\"],\n",
      "        \"level\": [\"level\", \"log_level\", \"severity\"],\n",
      "        \"timestamp\": [\"timestamp\", \"time\", \"date\", \"datetime\"],\n",
      "        \"action\": [\"action\", \"event\", \"msg\", \"message\"],\n",
      "        \"success\": [\"success\", \"status\", \"result\"],\n",
      "        \"unparsed\": [\"_unparsed\", \"unparsed\", \"raw\"],\n",
      "    }\n",
      "\n",
      "    # Regex to recognise monetary / numeric strings\n",
      "    _num_re = re.compile(r\"\"\"^\\s*\n",
      "        \\$?                # optional leading dollar sign\n",
      "        -?                 # optional negative sign\n",
      "        (\\d{1,3}(,\\d{3})*|\\d+)  # digits with optional commas\n",
      "        (\\.\\d+)?           # optional decimal part\n",
      "        \\s*$\"\"\", re.VERBOSE)\n",
      "\n",
      "    def _coerce(value: Any) -> Any:\n",
      "        \"\"\"Best‑effort conversion of a string to bool/float, otherwise leave unchanged.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            low = value.strip().lower()\n",
      "            if low in (\"true\", \"false\"):\n",
      "                return low == \"true\"\n",
      "            if _num_re.match(value):\n",
      "                # Remove commas and possible leading '$', then cast to float\n",
      "                cleaned = value.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
      "                try:\n",
      "                    return float(cleaned)\n",
      "                except ValueError:\n",
      "                    pass\n",
      "        return value\n",
      "\n",
      "    transformed: List[Dict[str, Optional[Any]]] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        dst: Dict[str, Optional[Any]] = {\n",
      "            \"user_id\": None,\n",
      "            \"user\": None,\n",
      "            \"level\": None,\n",
      "            \"timestamp\": None,\n",
      "            \"action\": None,\n",
      "            \"success\": False,   # default per schema\n",
      "            \"unparsed\": None,\n",
      "        }\n",
      "\n",
      "        for target, candidates in field_map.items():\n",
      "            for key in candidates:\n",
      "                if key in src and src[key] is not None:\n",
      "                    dst[target] = _coerce(src[key])\n",
      "                    # ``success`` defaults to ``False``; if we find a value we keep it\n",
      "                    break  # stop at first matching key\n",
      "\n",
      "        transformed.append(dst)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:46:11\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 3\n",
      "\u001b[38;5;245m2026-02-02 12:46:11\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 123.0, 'user': None, 'level': None, 'timestamp': None, 'action': None, 'success': False, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- FAILED\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that conform to the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * coalesces multiple possible source keys for each target field,\n",
      "    * infers simple data types (bool, int, float) from string values,\n",
      "    * supplies defaults (e.g. ``success`` defaults to ``False``),\n",
      "    * retains any raw line under the ``unparsed`` field when present.\n",
      "    \"\"\"\n",
      "    import re\n",
      "\n",
      "    # Mapping of target field -> possible source keys (ordered by priority)\n",
      "    FIELD_MAP = {\n",
      "        \"user_id\": [\"user_id\", \"id\"],\n",
      "        \"user\": [\"user\", \"username\", \"account_name\"],\n",
      "        \"level\": [\"level\", \"log_level\", \"severity\"],\n",
      "        \"timestamp\": [\"timestamp\", \"time\", \"date\", \"datetime\"],\n",
      "        \"action\": [\"action\", \"event\", \"msg\", \"message\"],\n",
      "        \"success\": [\"success\", \"result\", \"status\", \"outcome\"],\n",
      "        \"unparsed\": [\"unparsed\", \"_unparsed\", \"raw\", \"raw_line\"],\n",
      "    }\n",
      "\n",
      "    # Helper to coerce a string into a more appropriate python type\n",
      "    def infer_type(value):\n",
      "        if isinstance(value, (bool, int, float)):\n",
      "            return value\n",
      "        if not isinstance(value, str):\n",
      "            return value\n",
      "\n",
      "        stripped = value.strip()\n",
      "        lowered = stripped.lower()\n",
      "\n",
      "        # Boolean detection\n",
      "        if lowered in {\"true\", \"yes\", \"1\"}:\n",
      "            return True\n",
      "        if lowered in {\"false\", \"no\", \"0\"}:\n",
      "            return False\n",
      "\n",
      "        # Numeric detection (allow optional leading currency symbols)\n",
      "        # Remove common currency symbols and commas first\n",
      "        cleaned = stripped.lstrip(\"$€£¥\").replace(\",\", \"\")\n",
      "        num_match = re.fullmatch(r\"[+-]?\\d+(\\.\\d+)?\", cleaned)\n",
      "        if num_match:\n",
      "            try:\n",
      "                # Prefer int when possible\n",
      "                if \".\" not in cleaned:\n",
      "                    return int(cleaned)\n",
      "                return float(cleaned)\n",
      "            except ValueError:\n",
      "                pass  # fall back to original string\n",
      "\n",
      "        # Fallback – keep original string\n",
      "        return stripped\n",
      "\n",
      "    transformed = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        # start with defaults\n",
      "        entry = {\n",
      "            \"user_id\": None,\n",
      "            \"user\": None,\n",
      "            \"level\": None,\n",
      "            \"timestamp\": None,\n",
      "            \"action\": None,\n",
      "            \"success\": False,   # default per schema\n",
      "            \"unparsed\": None,\n",
      "        }\n",
      "\n",
      "        for target, candidates in FIELD_MAP.items():\n",
      "            for key in candidates:\n",
      "                if key in src and src[key] is not None:\n",
      "                    # special handling for the boolean ``success`` default\n",
      "                    if target == \"success\":\n",
      "                        entry[target] = bool(infer_type(src[key]))\n",
      "                    else:\n",
      "                        entry[target] = infer_type(src[key])\n",
      "                    break  # stop at first matching key\n",
      "\n",
      "        transformed.append(entry)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:46:22\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 4\n",
      "\u001b[38;5;245m2026-02-02 12:46:22\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 123, 'user': None, 'level': None, 'timestamp': None, 'action': None, 'success': False, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- FAILED\n",
      "```python\n",
      "import re\n",
      "from typing import Any, List, Dict, Optional\n",
      "\n",
      "def transform_to_models(parsed_dict: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
      "    \"\"\"\n",
      "    Convert a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * coalesces multiple possible source keys for each target field,\n",
      "    * infers simple data types from string values (bool, float),\n",
      "    * supplies defaults (``success`` → ``False``) when a field is missing.\n",
      "    \"\"\"\n",
      "    # Mapping of target field → possible source keys (ordered by priority)\n",
      "    field_map: Dict[str, List[str]] = {\n",
      "        \"user_id\": [\"user_id\", \"id\"],\n",
      "        \"user\": [\"user\", \"username\", \"account_name\"],\n",
      "        \"level\": [\"level\", \"log_level\", \"severity\"],\n",
      "        \"timestamp\": [\"timestamp\", \"time\", \"date\", \"datetime\"],\n",
      "        \"action\": [\"action\", \"event\", \"msg\", \"message\"],\n",
      "        \"success\": [\"success\", \"status\", \"result\"],\n",
      "        \"unparsed\": [\"_unparsed\", \"unparsed\", \"raw\"],\n",
      "    }\n",
      "\n",
      "    # Regex to recognise monetary / numeric strings\n",
      "    _num_re = re.compile(r\"\"\"^\\s*\n",
      "        \\$?                # optional leading dollar sign\n",
      "        -?                 # optional negative sign\n",
      "        (\\d{1,3}(,\\d{3})*|\\d+)  # digits with optional commas\n",
      "        (\\.\\d+)?           # optional decimal part\n",
      "        \\s*$\"\"\", re.VERBOSE)\n",
      "\n",
      "    def _coerce(value: Any) -> Any:\n",
      "        \"\"\"Best‑effort conversion of a string to bool/float, otherwise leave unchanged.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            low = value.strip().lower()\n",
      "            if low in (\"true\", \"false\"):\n",
      "                return low == \"true\"\n",
      "            if _num_re.match(value):\n",
      "                # Remove commas and possible leading '$', then cast to float\n",
      "                cleaned = value.replace(\"$\", \"\").replace(\",\", \"\").strip()\n",
      "                try:\n",
      "                    return float(cleaned)\n",
      "                except ValueError:\n",
      "                    pass\n",
      "        return value\n",
      "\n",
      "    transformed: List[Dict[str, Optional[Any]]] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        dst: Dict[str, Optional[Any]] = {\n",
      "            \"user_id\": None,\n",
      "            \"user\": None,\n",
      "            \"level\": None,\n",
      "            \"timestamp\": None,\n",
      "            \"action\": None,\n",
      "            \"success\": False,   # default per schema\n",
      "            \"unparsed\": None,\n",
      "        }\n",
      "\n",
      "        for target, candidates in field_map.items():\n",
      "            for key in candidates:\n",
      "                if key in src and src[key] is not None:\n",
      "                    dst[target] = _coerce(src[key])\n",
      "                    # ``success`` defaults to ``False``; if we find a value we keep it\n",
      "                    break  # stop at first matching key\n",
      "\n",
      "        transformed.append(dst)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:46:41\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 5\n",
      "\u001b[38;5;245m2026-02-02 12:46:41\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 123.0, 'user': None, 'level': None, 'timestamp': None, 'action': None, 'success': False, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- FAILED\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * coalesces multiple possible source keys for each target field,\n",
      "    * attempts to infer a more appropriate Python type from string values,\n",
      "    * supplies a default ``False`` for the ``success`` flag when missing,\n",
      "    * and returns plain ``dict`` objects (ready to be fed to the Pydantic model).\n",
      "    \"\"\"\n",
      "    # Mapping of target field -> possible source keys (ordered by priority)\n",
      "    field_map = {\n",
      "        \"user_id\": [\"user_id\", \"id\"],\n",
      "        \"user\": [\"user\", \"username\", \"account_name\"],\n",
      "        \"level\": [\"level\", \"log_level\", \"severity\"],\n",
      "        \"timestamp\": [\"timestamp\", \"time\", \"date\", \"datetime\"],\n",
      "        \"action\": [\"action\", \"event\", \"operation\", \"activity\"],\n",
      "        \"success\": [\"success\", \"result\", \"status\", \"outcome\"],\n",
      "        \"unparsed\": [\"_unparsed\", \"raw\", \"unparsed\"],\n",
      "    }\n",
      "\n",
      "    # Helper to coerce string values to a more natural Python type\n",
      "    def _coerce(val):\n",
      "        if isinstance(val, str):\n",
      "            v = val.strip()\n",
      "            # Boolean detection\n",
      "            lowered = v.lower()\n",
      "            if lowered in {\"true\", \"yes\", \"1\"}:\n",
      "                return True\n",
      "            if lowered in {\"false\", \"no\", \"0\"}:\n",
      "                return False\n",
      "            # Numeric detection (remove common currency symbols & commas)\n",
      "            try:\n",
      "                cleaned = v.replace(\"$\", \"\").replace(\"€\", \"\").replace(\"£\", \"\").replace(\",\", \"\")\n",
      "                if \".\" in cleaned:\n",
      "                    return float(cleaned)\n",
      "                return int(cleaned)\n",
      "            except ValueError:\n",
      "                pass\n",
      "        return val\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        out: dict = {}\n",
      "        for target, candidates in field_map.items():\n",
      "            # Find the first candidate key that exists in the source dict\n",
      "            raw_val = next((src.get(k) for k in candidates if k in src), None)\n",
      "\n",
      "            # Special handling for the ``success`` flag – default to False\n",
      "            if target == \"success\":\n",
      "                out[target] = bool(_coerce(raw_val)) if raw_val is not None else False\n",
      "                continue\n",
      "\n",
      "            # Preserve ``None`` when the field is truly absent\n",
      "            out[target] = _coerce(raw_val) if raw_val is not None else None\n",
      "\n",
      "        transformed.append(out)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:46:51\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 6\n",
      "\u001b[38;5;245m2026-02-02 12:46:51\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m1738028846.py:26\u001b[0m --- Finished Cluster 3 ---\n",
      "\u001b[38;5;245m2026-02-02 12:46:51\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:85\u001b[0m [Scehma Tester] Validating Functional BaseModel: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': 123, 'user': None, 'level': None, 'timestamp': None, 'action': None, 'success': False, 'unparsed': '2026-01-30 14:22:01 INFO User login successful'} -- FAILED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:46:51\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"Map loosely‑parsed log dictionaries to the LogEntry schema.\n",
      "\n",
      "    The function coalesces possible source keys for each target field,\n",
      "    converts the ``success`` flag to a proper boolean, and leaves all other\n",
      "    values as‑is (strings or ``None``) so they match the ``LogEntry`` model.\n",
      "    \"\"\"\n",
      "    # Helper: return the first non‑null value found for a list of possible keys\n",
      "    def _coalesce(src: dict, keys: list[str]):\n",
      "        for k in keys:\n",
      "            if k in src and src[k] is not None:\n",
      "                return src[k]\n",
      "        return None\n",
      "\n",
      "    # Helper: robust conversion of various truthy representations to bool\n",
      "    def _to_bool(val):\n",
      "        if isinstance(val, bool):\n",
      "            return val\n",
      "        if isinstance(val, (int, float)):\n",
      "            return bool(val)\n",
      "        if isinstance(val, str):\n",
      "            return val.strip().lower() in {\"true\", \"1\", \"yes\", \"y\", \"t\"}\n",
      "        return False\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "    for src in parsed_dict:\n",
      "        # Extract raw ``unparsed`` line if present; otherwise leave None\n",
      "        unparsed_val = _coalesce(src, [\"unparsed\"])\n",
      "\n",
      "        # Build the target dict using the coalesce logic\n",
      "        entry = {\n",
      "            \"user_id\": _coalesce(src, [\"user_id\", \"id\"]),\n",
      "            \"user\": _coalesce(src, [\"user\"]),\n",
      "            \"level\": _coalesce(src, [\"level\"]),\n",
      "            \"timestamp\": _coalesce(src, [\"timestamp\"]),\n",
      "            \"action\": _coalesce(src, [\"action\"]),\n",
      "            \"success\": _to_bool(_coalesce(src, [\"success\"])) if _coalesce(src, [\"success\"]) is not None else False,\n",
      "            \"unparsed\": unparsed_val,\n",
      "        }\n",
      "        transformed.append(entry)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:47:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 1\n",
      "\u001b[38;5;245m2026-02-02 12:47:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m1738028846.py:26\u001b[0m --- Finished Cluster 2 ---\n",
      "\u001b[38;5;245m2026-02-02 12:47:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:44\u001b[0m [Architect] Defining Semantic Goal: 0\n",
      "\u001b[38;5;245m2026-02-02 12:47:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[34;20mDEBUG\u001b[0m] \u001b[38;5;245m2336556215.py:48\u001b[0m [\n",
      "  {\n",
      "    \"_unparsed\": \"2026-01-30 14:22:01 INFO User login successful\"\n",
      "  },\n",
      "  {\n",
      "    \"_unparsed\": \"[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \\\"GET /health 1.1\\\" 200 16 \\\"-\\\" \\\"curl/8.14.1\\\"\"\n",
      "  },\n",
      "  {\n",
      "    \"_unparsed\": \"2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'user_id': '1', 'user': 'Sam', 'level': 'INFO', 'timestamp': None, 'action': None, 'success': False, 'unparsed': None} -- PASSED\n",
      "Input: {'user_id': None, 'user': 'alice', 'level': 'INFO', 'timestamp': '2026-01-30T14:22:01Z', 'action': 'login', 'success': True, 'unparsed': None} -- PASSED\n",
      "--> Exported: parser_LogEntry_struct_2.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:47:24\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:85\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "<string>:46: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "\u001b[38;5;245m2026-02-02 12:47:24\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class RequestInfo(BaseModel):\n",
      "    client_ip: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"IP address of the client making the request.\"\n",
      "    )\n",
      "    client_port: Optional[int] = Field(\n",
      "        None,\n",
      "        description=\"Port number of the client.\"\n",
      "    )\n",
      "    request_time: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Timestamp of the request as recorded in the log entry.\"\n",
      "    )\n",
      "    request_method: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"HTTP method used in the request, e.g., GET, POST.\"\n",
      "    )\n",
      "    request_path: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Path component of the HTTP request.\"\n",
      "    )\n",
      "    request_protocol: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"Protocol version of the HTTP request, e.g., 1.1.\"\n",
      "    )\n",
      "    status_code: Optional[int] = Field(\n",
      "        None,\n",
      "        description=\"HTTP response status code returned to the client.\"\n",
      "    )\n",
      "    response_size: Optional[int] = Field(\n",
      "        None,\n",
      "        description=\"Size of the response payload in bytes.\"\n",
      "    )\n",
      "    referer: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"HTTP referer header value, if present.\"\n",
      "    )\n",
      "    user_agent: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"User‑Agent string identifying the client software.\"\n",
      "    )\n",
      "\n",
      "class LogEntry(BaseModel):\n",
      "    raw_timestamp: str = Field(\n",
      "        ..., description=\"Original timestamp string from the log line.\"\n",
      "    )\n",
      "    log_level: str = Field(\n",
      "        ..., description=\"Severity level of the log entry, e.g., INFO, DEBUG, WARN.\"\n",
      "    )\n",
      "    message: str = Field(\n",
      "        ..., description=\"Human‑readable message describing the event.\"\n",
      "    )\n",
      "    component: Optional[str] = Field(\n",
      "        None, description=\"Logical component or service name emitting the log.\"\n",
      "    )\n",
      "    process_id: Optional[int] = Field(\n",
      "        None, description=\"Identifier of the process that generated the log entry.\"\n",
      "    )\n",
      "    source_file: Optional[str] = Field(\n",
      "        None, description=\"Source code file where the log call originated.\"\n",
      "    )\n",
      "    source_line: Optional[int] = Field(\n",
      "        None, description=\"Line number within the source file.\"\n",
      "    )\n",
      "    request: Optional[RequestInfo] = Field(\n",
      "        None, description=\"Embedded HTTP request/response details when the log pertains to a web request.\"\n",
      "    )\n",
      "\n",
      "    class Config:\n",
      "        title = \"GoldStandardLogEntry\"\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Convert a list of loosely‑parsed log dictionaries into a list of dictionaries\n",
      "    that match the ``LogEntry`` Pydantic model schema.\n",
      "\n",
      "    The function follows a *coalesce* strategy – for each target field it checks a\n",
      "    prioritized list of possible source keys.  It also attempts to infer the\n",
      "    correct Python type (int, bool, etc.) for fields that expect non‑string\n",
      "    values.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    parsed_dict: list[dict]\n",
      "        Each element is a dictionary produced by a generic log‑parsing step.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    list[dict]\n",
      "        A list of dictionaries ready to be passed to ``LogEntry(**item)``.\n",
      "    \"\"\"\n",
      "\n",
      "    # --------------------------------------------------------------------- #\n",
      "    # Helper utilities\n",
      "    # --------------------------------------------------------------------- #\n",
      "    def _coalesce(src: dict, candidates: list[str]) -> object:\n",
      "        \"\"\"Return the first non‑None value found for the given candidate keys.\"\"\"\n",
      "        for key in candidates:\n",
      "            if key in src and src[key] is not None:\n",
      "                return src[key]\n",
      "        return None\n",
      "\n",
      "    def _to_int(val: object) -> int | None:\n",
      "        \"\"\"Convert ``val`` to int when possible, otherwise return None.\"\"\"\n",
      "        if isinstance(val, int):\n",
      "            return val\n",
      "        if isinstance(val, str):\n",
      "            try:\n",
      "                return int(val)\n",
      "            except ValueError:\n",
      "                # handle strings like \"1234 \" or \"12,34\"\n",
      "                stripped = val.strip().replace(\",\", \"\")\n",
      "                if stripped.isdigit() or (stripped.startswith(\"-\") and stripped[1:].isdigit()):\n",
      "                    try:\n",
      "                        return int(stripped)\n",
      "                    except ValueError:\n",
      "                        pass\n",
      "        return None\n",
      "\n",
      "    def _to_bool(val: object) -> bool | None:\n",
      "        \"\"\"Convert common truthy/falsey strings to bool.\"\"\"\n",
      "        if isinstance(val, bool):\n",
      "            return val\n",
      "        if isinstance(val, str):\n",
      "            lowered = val.strip().lower()\n",
      "            if lowered in {\"true\", \"yes\", \"1\"}:\n",
      "                return True\n",
      "            if lowered in {\"false\", \"no\", \"0\"}:\n",
      "                return False\n",
      "        return None\n",
      "\n",
      "    # --------------------------------------------------------------------- #\n",
      "    # Mapping tables: target field -> list of possible source keys (ordered)\n",
      "    # --------------------------------------------------------------------- #\n",
      "    LOG_ENTRY_MAP = {\n",
      "        \"raw_timestamp\": [\"raw_timestamp\", \"timestamp\", \"time\", \"date\", \"_unparsed\"],\n",
      "        \"log_level\":     [\"log_level\", \"level\", \"severity\"],\n",
      "        \"message\":       [\"message\", \"msg\", \"log_message\", \"_unparsed\"],\n",
      "        \"component\":     [\"component\", \"service\", \"module\", \"logger\"],\n",
      "        \"process_id\":    [\"process_id\", \"pid\", \"process\", \"processId\"],\n",
      "        \"source_file\":   [\"source_file\", \"file\", \"filename\", \"source\"],\n",
      "        \"source_line\":   [\"source_line\", \"line\", \"lineno\", \"line_number\"],\n",
      "    }\n",
      "\n",
      "    REQUEST_MAP = {\n",
      "        \"client_ip\":          [\"client_ip\", \"ip\", \"remote_addr\", \"host\"],\n",
      "        \"client_port\":       [\"client_port\", \"port\", \"remote_port\"],\n",
      "        \"request_time\":       [\"request_time\", \"request_timestamp\", \"time\", \"timestamp\"],\n",
      "        \"request_method\":    [\"request_method\", \"method\", \"http_method\"],\n",
      "        \"request_path\":      [\"request_path\", \"path\", \"url\"],\n",
      "        \"request_protocol\":  [\"request_protocol\", \"protocol\", \"http_version\"],\n",
      "        \"status_code\":       [\"status_code\", \"status\", \"http_status\"],\n",
      "        \"response_size\":     [\"response_size\", \"size\", \"bytes_sent\", \"content_length\"],\n",
      "        \"referer\":           [\"referer\", \"referrer\", \"http_referer\"],\n",
      "        \"user_agent\":        [\"user_agent\", \"ua\", \"user-agent\", \"http_user_agent\"],\n",
      "    }\n",
      "\n",
      "    # --------------------------------------------------------------------- #\n",
      "    # Main transformation loop\n",
      "    # --------------------------------------------------------------------- #\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        entry: dict = {}\n",
      "\n",
      "        # ----- LogEntry fields ------------------------------------------------\n",
      "        for target, candidates in LOG_ENTRY_MAP.items():\n",
      "            val = _coalesce(src, candidates)\n",
      "            if target in {\"process_id\", \"source_line\"}:\n",
      "                val = _to_int(val)\n",
      "            entry[target] = val\n",
      "\n",
      "        # ----- RequestInfo (nested) -------------------------------------------\n",
      "        request_fields: dict = {}\n",
      "        for target, candidates in REQUEST_MAP.items():\n",
      "            val = _coalesce(src, candidates)\n",
      "            if val is None:\n",
      "                continue\n",
      "\n",
      "            # Type coercion for known numeric / boolean fields\n",
      "            if target in {\"client_port\", \"status_code\", \"response_size\"}:\n",
      "                val = _to_int(val)\n",
      "            elif target in {\"client_ip\", \"client_port\", \"request_time\",\n",
      "                            \"request_method\", \"request_path\", \"request_protocol\",\n",
      "                            \"referer\", \"user_agent\"}:\n",
      "                # keep as string (ensure it's really a string)\n",
      "                val = str(val)\n",
      "\n",
      "            request_fields[target] = val\n",
      "\n",
      "        # Only attach the request dict if we actually captured something\n",
      "        entry[\"request\"] = request_fields if request_fields else None\n",
      "\n",
      "        # The Pydantic model requires ``raw_timestamp``, ``log_level`` and ``message``.\n",
      "        # If any of those are still None we fallback to an empty string to avoid validation errors.\n",
      "        for mandatory in (\"raw_timestamp\", \"log_level\", \"message\"):\n",
      "            if entry.get(mandatory) is None:\n",
      "                entry[mandatory] = \"\"\n",
      "\n",
      "        transformed.append(entry)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:47:59\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'raw_timestamp': '2026-01-30 14:22:01 INFO User login successful', 'log_level': '', 'message': '2026-01-30 14:22:01 INFO User login successful', 'component': None, 'process_id': None, 'source_file': None, 'source_line': None, 'request': None} -- PASSED\n",
      "Input: {'raw_timestamp': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"', 'log_level': '', 'message': '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"', 'component': None, 'process_id': None, 'source_file': None, 'source_line': None, 'request': None} -- PASSED\n",
      "Input: {'raw_timestamp': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check', 'log_level': '', 'message': '2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check', 'component': None, 'process_id': None, 'source_file': None, 'source_line': None, 'request': None} -- PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:51: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "\u001b[38;5;245m2026-02-02 12:47:59\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m1738028846.py:26\u001b[0m --- Finished Cluster 3 ---\n",
      "\u001b[38;5;245m2026-02-02 12:47:59\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:44\u001b[0m [Architect] Defining Semantic Goal: 0\n",
      "\u001b[38;5;245m2026-02-02 12:47:59\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[34;20mDEBUG\u001b[0m] \u001b[38;5;245m2336556215.py:48\u001b[0m [\n",
      "  {\n",
      "    \"level\": \"INFO\",\n",
      "    \"cpu_usage\": \"1,234.56\",\n",
      "    \"memory\": \"512MB\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Exported: parser_LogEntry_struct_3.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:48:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:85\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 12:48:07\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class LogMetrics(BaseModel):\n",
      "    level: str = Field(..., description=\"Log level indicator, e.g., 'INFO', 'DEBUG', 'WARN', 'ERROR'.\")\n",
      "    cpu_usage: float = Field(..., description=\"CPU usage as a percentage, represented as a float. The original string may contain commas as thousand separators.\")\n",
      "    memory: str = Field(..., description=\"Memory usage string including numeric value and unit, e.g., '512MB'.\")\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries that match\n",
      "    the LogMetrics Pydantic model schema.\n",
      "\n",
      "    The function uses a *coalesce* strategy: for each target field it checks a prioritized\n",
      "    list of possible source keys and picks the first one that exists in the input dict.\n",
      "    It also performs minimal type inference/clean‑up (e.g., removing commas from numbers).\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    parsed_dict: list[dict]\n",
      "        List of dictionaries obtained from raw parsing.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    list[dict]\n",
      "        List of dictionaries ready to be fed into ``LogMetrics(**item)``.\n",
      "    \"\"\"\n",
      "    # Possible source keys for each target field (ordered by priority)\n",
      "    FIELD_MAP = {\n",
      "        \"level\": [\"level\", \"log_level\", \"severity\", \"loglevel\"],\n",
      "        \"cpu_usage\": [\"cpu_usage\", \"cpu\", \"cpu%\", \"cpu_usage_percent\"],\n",
      "        \"memory\": [\"memory\", \"mem\", \"ram\", \"memory_usage\"],\n",
      "    }\n",
      "\n",
      "    def _coalesce(source: dict, candidates: list[str]):\n",
      "        \"\"\"Return the first non‑None value for the given candidate keys.\"\"\"\n",
      "        for key in candidates:\n",
      "            if key in source:\n",
      "                return source.get(key)\n",
      "        return None\n",
      "\n",
      "    def _infer_type(value):\n",
      "        \"\"\"Best‑effort conversion of a string to int, float, bool or leave as‑is.\"\"\"\n",
      "        if isinstance(value, (int, float, bool)):\n",
      "            return value\n",
      "        if not isinstance(value, str):\n",
      "            return value\n",
      "\n",
      "        lowered = value.strip().lower()\n",
      "        # Boolean detection\n",
      "        if lowered in {\"true\", \"false\"}:\n",
      "            return lowered == \"true\"\n",
      "\n",
      "        # Integer detection (no decimal point)\n",
      "        try:\n",
      "            if lowered.isdigit() or (lowered.startswith(\"-\") and lowered[1:].isdigit()):\n",
      "                return int(lowered)\n",
      "        except Exception:\n",
      "            pass\n",
      "\n",
      "        # Float detection (including commas as thousand separators)\n",
      "        try:\n",
      "            # Remove commas used as thousand separators\n",
      "            cleaned = lowered.replace(\",\", \"\")\n",
      "            return float(cleaned)\n",
      "        except Exception:\n",
      "            pass\n",
      "\n",
      "        # Fallback – keep original string\n",
      "        return value\n",
      "\n",
      "    transformed = []\n",
      "    for src in parsed_dict:\n",
      "        # Level – keep as string (no special conversion needed)\n",
      "        level_val = _coalesce(src, FIELD_MAP[\"level\"])\n",
      "        level = str(level_val) if level_val is not None else None\n",
      "\n",
      "        # CPU usage – must be a float; strip commas and convert\n",
      "        cpu_raw = _coalesce(src, FIELD_MAP[\"cpu_usage\"])\n",
      "        cpu_usage = None\n",
      "        if cpu_raw is not None:\n",
      "            # Use the generic inference first, then enforce float\n",
      "            inferred = _infer_type(cpu_raw)\n",
      "            try:\n",
      "                cpu_usage = float(inferred)\n",
      "            except Exception:\n",
      "                # If conversion fails, raise a clear error later by leaving None\n",
      "                cpu_usage = None\n",
      "\n",
      "        # Memory – keep as string (ensure it's a string)\n",
      "        mem_raw = _coalesce(src, FIELD_MAP[\"memory\"])\n",
      "        memory = str(mem_raw) if mem_raw is not None else None\n",
      "\n",
      "        transformed.append({\n",
      "            \"level\": level,\n",
      "            \"cpu_usage\": cpu_usage,\n",
      "            \"memory\": memory,\n",
      "        })\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:48:23\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'level': 'INFO', 'cpu_usage': 1234.56, 'memory': '512MB'} -- PASSED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:48:23\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m1738028846.py:26\u001b[0m --- Finished Cluster 2 ---\n",
      "\u001b[38;5;245m2026-02-02 12:48:23\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:44\u001b[0m [Architect] Defining Semantic Goal: 0\n",
      "\u001b[38;5;245m2026-02-02 12:48:23\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[34;20mDEBUG\u001b[0m] \u001b[38;5;245m2336556215.py:48\u001b[0m [\n",
      "  {\n",
      "    \"id\": \"usr_001\",\n",
      "    \"name\": \"Alex Johnson\",\n",
      "    \"role\": \"admin\",\n",
      "    \"isActive\": true,\n",
      "    \"createdAt\": \"2025-11-02T09:14:23Z\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"usr_002\",\n",
      "    \"name\": \"Maria Lopez\",\n",
      "    \"email\": \"maria.lopez@example.com\",\n",
      "    \"role\": \"editor\",\n",
      "    \"isActive\": null,\n",
      "    \"createdAt\": \"2025-12-18T16:47:10Z\",\n",
      "    \"lastLoginIp\": \"192.168.1.42\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"usr_003\",\n",
      "    \"email\": \"samir.patel@example.com\",\n",
      "    \"role\": \"viewer\",\n",
      "    \"isActive\": false,\n",
      "    \"createdAt\": \"08/05/2024\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"name\": \"Chen Wei\",\n",
      "    \"email\": \"chen.wei@example.com\",\n",
      "    \"isActive\": true,\n",
      "    \"createdAt\": null\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"usr_005\",\n",
      "    \"name\": \"Broken Record\",\n",
      "    \"email\": \"broken@example.com\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Exported: parser_LogMetrics_struct_2.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:48:33\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:85\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 12:48:33\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "class UserBase(BaseModel):\n",
      "    \"\"\"\n",
      "    Gold‑standard representation of a user entity.\n",
      "    \"\"\"\n",
      "    id: str = Field(..., description=\"Primary identifier of the user, kept as a string.\")\n",
      "    name: Optional[str] = Field(None, description=\"Full name of the user.\")\n",
      "    email: Optional[str] = Field(None, description=\"User's email address.\")\n",
      "    role: Optional[str] = Field(None, description=\"Role assigned to the user (e.g., admin, editor, viewer).\")\n",
      "    is_active: bool = Field(\n",
      "        default=False,\n",
      "        description=\"Flag indicating whether the user account is currently active.\"\n",
      "    )\n",
      "    created_at: Optional[str] = Field(\n",
      "        None,\n",
      "        description=(\n",
      "            \"Timestamp when the user record was created. \"\n",
      "            \"Accepts ISO‑8601 strings or other common date representations.\"\n",
      "        )\n",
      "    )\n",
      "    last_login_ip: Optional[str] = Field(\n",
      "        None,\n",
      "        description=\"IP address from which the user last logged in.\"\n",
      "    )\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Convert a list of loosely‑structured dictionaries into a list of dictionaries that\n",
      "    conform to the ``UserBase`` Pydantic model schema.\n",
      "\n",
      "    The function follows a *coalesce* strategy – for each target field it checks a\n",
      "    prioritized list of possible source keys and picks the first non‑null value.\n",
      "    Simple type‑inference is applied so that strings like \"true\", \"false\",\n",
      "    \"$120.00\", \"1\", etc. become proper ``bool`` or ``float`` values.\n",
      "    \"\"\"\n",
      "\n",
      "    from typing import Any, List, Dict\n",
      "\n",
      "    # ------------------------------------------------------------------ #\n",
      "    # Helper: pick first non‑None value from a list of possible keys.\n",
      "    # ------------------------------------------------------------------ #\n",
      "    def coalesce(source: Dict[str, Any], keys: List[str]) -> Any:\n",
      "        for k in keys:\n",
      "            if k in source and source[k] is not None:\n",
      "                return source[k]\n",
      "        return None\n",
      "\n",
      "    # ------------------------------------------------------------------ #\n",
      "    # Helper: infer a Python primitive from a string (or pass‑through otherwise).\n",
      "    # ------------------------------------------------------------------ #\n",
      "    def infer_primitive(value: Any) -> Any:\n",
      "        if isinstance(value, str):\n",
      "            v = value.strip()\n",
      "\n",
      "            # Boolean strings\n",
      "            low = v.lower()\n",
      "            if low in {\"true\", \"t\", \"yes\", \"y\", \"1\"}:\n",
      "                return True\n",
      "            if low in {\"false\", \"f\", \"no\", \"n\", \"0\"}:\n",
      "                return False\n",
      "\n",
      "            # Currency / numeric strings\n",
      "            # Remove common currency symbols and commas\n",
      "            cleaned = v.replace(\"$\", \"\").replace(\"£\", \"\").replace(\"€\", \"\").replace(\",\", \"\")\n",
      "            try:\n",
      "                if \".\" in cleaned:\n",
      "                    return float(cleaned)\n",
      "                else:\n",
      "                    return int(cleaned)\n",
      "            except ValueError:\n",
      "                pass  # not a pure number, keep as string\n",
      "\n",
      "        # Already a proper primitive (bool, int, float, etc.)\n",
      "        return value\n",
      "\n",
      "    # ------------------------------------------------------------------ #\n",
      "    # Mapping configuration: target field -> list of possible source keys\n",
      "    # ------------------------------------------------------------------ #\n",
      "    FIELD_MAP = {\n",
      "        \"id\": [\"id\", \"user_id\", \"uid\"],\n",
      "        \"name\": [\"name\", \"full_name\", \"username\"],\n",
      "        \"email\": [\"email\", \"email_address\", \"mail\"],\n",
      "        \"role\": [\"role\", \"user_role\", \"type\"],\n",
      "        \"is_active\": [\"isActive\", \"is_active\", \"active\", \"status\"],\n",
      "        \"created_at\": [\"createdAt\", \"created_at\", \"created\", \"timestamp\"],\n",
      "        \"last_login_ip\": [\"lastLoginIp\", \"last_login_ip\", \"lastIp\", \"login_ip\"],\n",
      "    }\n",
      "\n",
      "    transformed: List[Dict[str, Any]] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        # Build a dict that matches the UserBase schema\n",
      "        out: Dict[str, Any] = {}\n",
      "\n",
      "        # id – always present, force to string\n",
      "        raw_id = coalesce(src, FIELD_MAP[\"id\"])\n",
      "        out[\"id\"] = str(raw_id) if raw_id is not None else \"\"\n",
      "\n",
      "        # Optional fields – use .get‑style coalesce and infer types where appropriate\n",
      "        for field in (\"name\", \"email\", \"role\"):\n",
      "            raw = coalesce(src, FIELD_MAP[field])\n",
      "            out[field] = raw if raw is not None else None\n",
      "\n",
      "        # is_active – default False if missing; coalesce then infer bool\n",
      "        raw_active = coalesce(src, FIELD_MAP[\"is_active\"])\n",
      "        if raw_active is None:\n",
      "            out[\"is_active\"] = False\n",
      "        else:\n",
      "            out[\"is_active\"] = bool(infer_primitive(raw_active))\n",
      "\n",
      "        # created_at – keep as string (or None)\n",
      "        raw_created = coalesce(src, FIELD_MAP[\"created_at\"])\n",
      "        out[\"created_at\"] = raw_created if raw_created is not None else None\n",
      "\n",
      "        # last_login_ip – optional string\n",
      "        raw_ip = coalesce(src, FIELD_MAP[\"last_login_ip\"])\n",
      "        out[\"last_login_ip\"] = raw_ip if raw_ip is not None else None\n",
      "\n",
      "        transformed.append(out)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:48:55\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 2\n",
      "\u001b[38;5;245m2026-02-02 12:48:55\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m1738028846.py:26\u001b[0m --- Finished Cluster 0 ---\n",
      "\u001b[38;5;245m2026-02-02 12:48:55\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:44\u001b[0m [Architect] Defining Semantic Goal: 0\n",
      "\u001b[38;5;245m2026-02-02 12:48:55\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[34;20mDEBUG\u001b[0m] \u001b[38;5;245m2336556215.py:48\u001b[0m [\n",
      "  {\n",
      "    \"level\": \"INFO\",\n",
      "    \"service\": \"orders\",\n",
      "    \"order_id\": 1001,\n",
      "    \"status\": \"created\"\n",
      "  }\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'id': 'usr_001', 'name': 'Alex Johnson', 'email': None, 'role': 'admin', 'is_active': True, 'created_at': '2025-11-02T09:14:23Z', 'last_login_ip': None} -- PASSED\n",
      "Input: {'id': 'usr_002', 'name': 'Maria Lopez', 'email': 'maria.lopez@example.com', 'role': 'editor', 'is_active': False, 'created_at': '2025-12-18T16:47:10Z', 'last_login_ip': '192.168.1.42'} -- PASSED\n",
      "Input: {'id': 'usr_003', 'name': None, 'email': 'samir.patel@example.com', 'role': 'viewer', 'is_active': False, 'created_at': '08/05/2024', 'last_login_ip': None} -- PASSED\n",
      "Input: {'id': '4', 'name': 'Chen Wei', 'email': 'chen.wei@example.com', 'role': None, 'is_active': True, 'created_at': None, 'last_login_ip': None} -- PASSED\n",
      "Input: {'id': 'usr_005', 'name': 'Broken Record', 'email': 'broken@example.com', 'role': None, 'is_active': False, 'created_at': None, 'last_login_ip': None} -- PASSED\n",
      "--> Exported: parser_UserBase_struct_0.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:49:02\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:85\u001b[0m [Scehma Tester] Validating Functional BaseModel: 1\n",
      "\u001b[38;5;245m2026-02-02 12:49:02\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import List, Dict, Optional\n",
      "from pydantic import BaseModel, Field\n",
      "\n",
      "\n",
      "class OrderLogEntry(BaseModel):\n",
      "    \"\"\"Gold‑standard representation of a log entry emitted by the orders service.\n",
      "\n",
      "    The model captures the minimal, semantically‑rich fields observed across\n",
      "    sample payloads while preserving clear, snake_case naming.\n",
      "    \"\"\"\n",
      "\n",
      "    level: str = Field(\n",
      "        ..., description=\"Severity level of the log entry (e.g., 'INFO', 'WARN', 'ERROR').\"\n",
      "    )\n",
      "    service: str = Field(\n",
      "        ..., description=\"Name of the service that generated the log entry.\"\n",
      "    )\n",
      "    order_id: str = Field(\n",
      "        ..., description=\"Unique identifier of the order associated with this event. Stored as a string to preserve primary‑key semantics.\"\n",
      "    )\n",
      "    status: str = Field(\n",
      "        ..., description=\"Current status of the order (e.g., 'created', 'processed', 'shipped').\"\n",
      "    )\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Convert a list of loosely‑structured dictionaries into a list of dictionaries\n",
      "    that match the ``OrderLogEntry`` Pydantic model.\n",
      "\n",
      "    The function:\n",
      "    * coalesces possible source keys for each target field,\n",
      "    * coerces string values to their most appropriate Python type\n",
      "      (int, float, bool, or leaves as‑is),\n",
      "    * returns a list of clean dicts ready for ``OrderLogEntry(**item)``.\n",
      "    \"\"\"\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # Helper: best‑effort type inference for scalar values\n",
      "    # ------------------------------------------------------------------\n",
      "    def _coerce(value):\n",
      "        \"\"\"Infer int, float, bool from a string; otherwise return original.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            v = value.strip()\n",
      "            # Boolean\n",
      "            low = v.lower()\n",
      "            if low in {\"true\", \"false\"}:\n",
      "                return low == \"true\"\n",
      "            # Integer (including negative)\n",
      "            if v.isdigit() or (v.startswith(\"-\") and v[1:].isdigit()):\n",
      "                try:\n",
      "                    return int(v)\n",
      "                except ValueError:\n",
      "                    pass\n",
      "            # Float – strip common currency symbols and commas\n",
      "            try:\n",
      "                cleaned = v.replace(\"$\", \"\").replace(\",\", \"\")\n",
      "                return float(cleaned)\n",
      "            except ValueError:\n",
      "                pass\n",
      "        return value\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # Mapping: target field -> list of possible source keys (ordered by priority)\n",
      "    # ------------------------------------------------------------------\n",
      "    field_map = {\n",
      "        \"level\": [\"level\", \"lvl\", \"severity\"],\n",
      "        \"service\": [\"service\", \"svc\", \"source\"],\n",
      "        \"order_id\": [\"order_id\", \"orderId\", \"order-id\", \"id\", \"order\"],\n",
      "        \"status\": [\"status\", \"state\", \"order_status\"],\n",
      "    }\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for raw in parsed_dict:\n",
      "        # Build a clean dict for the target model\n",
      "        clean_entry: dict = {}\n",
      "        for target, candidates in field_map.items():\n",
      "            # Coalesce: first non‑None value among possible keys\n",
      "            value = None\n",
      "            for key in candidates:\n",
      "                if key in raw:\n",
      "                    value = raw.get(key)\n",
      "                    break\n",
      "            # Apply coercion if a value was found\n",
      "            clean_entry[target] = _coerce(value) if value is not None else None\n",
      "        transformed.append(clean_entry)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:49:09\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 2\n",
      "\u001b[38;5;245m2026-02-02 12:49:09\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:105\u001b[0m [Coder] Parser for Gold Schema: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'level': 'INFO', 'service': 'orders', 'order_id': 1001, 'status': 'created'} -- FAILED\n",
      "```python\n",
      "def transform_to_models(parsed_dict: list[dict]) -> list[dict]:\n",
      "    \"\"\"\n",
      "    Transform a list of loosely‑parsed dictionaries into a list of dictionaries\n",
      "    that conform to the ``OrderLogEntry`` schema.\n",
      "\n",
      "    The function:\n",
      "    * Coalesces possible source keys for each target field.\n",
      "    * Casts values to their most appropriate Python type.\n",
      "    * Ensures ``order_id`` is emitted as a string.\n",
      "    \"\"\"\n",
      "    from typing import Any\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # Helper: best‑effort type inference / coercion\n",
      "    # ------------------------------------------------------------------\n",
      "    def _coerce(value: Any) -> Any:\n",
      "        \"\"\"Convert strings that look like numbers, booleans or currency to native types.\"\"\"\n",
      "        if isinstance(value, str):\n",
      "            v = value.strip()\n",
      "            # Boolean strings\n",
      "            low = v.lower()\n",
      "            if low in {\"true\", \"false\"}:\n",
      "                return low == \"true\"\n",
      "            # Currency / monetary values like \"$120.00\" or \"€99\"\n",
      "            if v and v[0] in \"$€£¥\":\n",
      "                try:\n",
      "                    return float(v[1:].replace(\",\", \"\"))\n",
      "                except ValueError:\n",
      "                    pass\n",
      "            # Integer\n",
      "            try:\n",
      "                return int(v.replace(\",\", \"\"))\n",
      "            except ValueError:\n",
      "                pass\n",
      "            # Float\n",
      "            try:\n",
      "                return float(v.replace(\",\", \"\"))\n",
      "            except ValueError:\n",
      "                pass\n",
      "        return value\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # Mapping of target fields to possible source keys (coalesce order)\n",
      "    # ------------------------------------------------------------------\n",
      "    field_key_map = {\n",
      "        \"level\": [\"level\", \"log_level\", \"severity\"],\n",
      "        \"service\": [\"service\", \"svc\", \"service_name\"],\n",
      "        \"order_id\": [\"order_id\", \"orderId\", \"id\", \"order-id\"],\n",
      "        \"status\": [\"status\", \"order_status\", \"state\"],\n",
      "    }\n",
      "\n",
      "    transformed: list[dict] = []\n",
      "\n",
      "    for src in parsed_dict:\n",
      "        out: dict = {}\n",
      "        for target, possible_keys in field_key_map.items():\n",
      "            # Find the first key that exists in the source dict\n",
      "            raw_val = None\n",
      "            for key in possible_keys:\n",
      "                if key in src:\n",
      "                    raw_val = src.get(key)\n",
      "                    break\n",
      "            # If the field is missing, skip (Pydantic will raise on validation later)\n",
      "            if raw_val is None:\n",
      "                continue\n",
      "\n",
      "            # Coerce to appropriate type\n",
      "            val = _coerce(raw_val)\n",
      "\n",
      "            # ``order_id`` must be a string per the schema\n",
      "            if target == \"order_id\":\n",
      "                val = str(val)\n",
      "\n",
      "            out[target] = val\n",
      "        transformed.append(out)\n",
      "\n",
      "    return transformed\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;245m2026-02-02 12:49:18\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m2336556215.py:138\u001b[0m [Code Tester] Stress-testing parser: 3\n",
      "\u001b[38;5;245m2026-02-02 12:49:18\u001b[0m \u001b[36;20mswirl\u001b[0m [\u001b[32;20mINFO\u001b[0m] \u001b[38;5;245m1738028846.py:26\u001b[0m --- Finished Cluster 3 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'level': 'INFO', 'service': 'orders', 'order_id': '1001', 'status': 'created'} -- PASSED\n",
      "--> Exported: parser_OrderLogEntry_struct_3.py\n"
     ]
    }
   ],
   "source": [
    "async def run_data_matrix(all_data: Dict[int, List[Dict]]):\n",
    "    for sem_id, records in all_data.items():\n",
    "        struct_groups = {}\n",
    "        for s in records:\n",
    "            cid = s[\"structure_cluster_id\"]\n",
    "            struct_groups.setdefault(cid, []).append(s)\n",
    "\n",
    "        shared_gold_schema = None\n",
    "\n",
    "        for struct_id, pairs in struct_groups.items():\n",
    "            config = {\"configurable\": {\"thread_id\": f\"sem_{sem_id}_str_{struct_id}\"}}\n",
    "\n",
    "            # Fix the typo 'structure_causter_id' -> 'structure_cluster_id'\n",
    "            initial_state = {\n",
    "                \"semantic_id\": str(sem_id),\n",
    "                \"structure_cluster_id\": str(struct_id),\n",
    "                \"data_pairs_all\": records,\n",
    "                \"data_pairs_structure\": pairs,\n",
    "                \"gold_schema\": shared_gold_schema,\n",
    "                \"attempts\": 0,\n",
    "            }\n",
    "\n",
    "            final_output = await app.ainvoke(initial_state, config)\n",
    "            shared_gold_schema = final_output.get(\"gold_schema\")\n",
    "\n",
    "            logger.info(f\"--- Finished Cluster {struct_id} ---\")\n",
    "\n",
    "\n",
    "await run_data_matrix(cluster_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36ef94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dq-swirl (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
