{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebfeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import tqdm as notebook_tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import HDBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "from dq_swirl.clients.async_llm_client import AsyncLLMClient\n",
    "from dq_swirl.ingestion.rust_ingestion import smart_parse_batch\n",
    "from dq_swirl.ingestion.structure_analyzer import StructuralAnalyzer\n",
    "\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../secrets.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067ba83",
   "metadata": {},
   "source": [
    "## Messy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9549bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_data = [\n",
    "    \"Order 1001: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
    "    \"Order 1004:   Buyer=  AMANDA SMITH ,Location=Seattle, WA,Total=$50.00, Items: desk lamp\",\n",
    "    \"Order 1005: Buyer=Raj Patel, Total=1,200.50, Items: monitor, stand, cable\",\n",
    "    \"Order 1006: total=$89.99, location=Miami, FL, buyer=Elena Rossi, Items: keyboard\",\n",
    "    \"Order 1007: Buyer=Chris P., Location=Denver, CO, Total=$12.00, Items: stickers -- [DISCOUNT APPLIED]\",\n",
    "    \"Order 1008: Buyer=O'Connor, S., Location=Portland, OR, Total=$0.00, Items: \",\n",
    "    \"Order 1011: Buyer=John Davis, Location=Columbus, OH, Total=$742.10, Items: laptop, hdmi cable\",\n",
    "    \"Order 1012: Buyer=Sarah Liu, Location=Austin, TX, Total=$156.55, Items: headphones\",\n",
    "    \"Order 1013: Buyer=Mike Turner, Location=Cleveland, OH, Total=$1299.99, Items: gaming pc, mouse\",\n",
    "    \"Order 1014: Buyer=Rachel Kim, Locadtion=Seattle, WA, Total=$89.50, Items: coffee maker\",\n",
    "    \"Order 1015: Buyer=Chris Myers, Location=Cincinnati, OH, Total=$512.00, Items: monitor, desk lamp\",\n",
    "    \"Order=1016, Buyer=Jake Myers, Total=$1,512.00, Items: monitor,\",\n",
    "    '{\"id\": \"usr_001\", \"name\": \"Alex Johnson\", \"role\": \"admin\", \"isActive\": true, \"createdAt\": \"2025-11-02T09:14:23Z\"}',\n",
    "    '{\"id\": \"usr_002\", \"name\": \"Maria Lopez\", \"email\": \"maria.lopez@example.com\", \"role\": \"editor\", \"isActive\": null, \"createdAt\": \"2025-12-18T16:47:10Z\", \"lastLoginIp\": \"192.168.1.42\"}',\n",
    "    '{\"id\": \"usr_003\", \"email\": \"samir.patel@example.com\", \"role\": \"viewer\", \"isActive\": false, \"createdAt\": \"08/05/2024\"}',\n",
    "    '{\"id\": 4, \"name\": \"Chen Wei\", \"email\": \"chen.wei@example.com\", \"isActive\": true, \"createdAt\": null}',\n",
    "    '{\"id\": \"usr_005\", \"name\": \"Broken Record\", \"email\": \"broken@example.com\"}',\n",
    "    \"Order 1017: Buyer=Griffin Arora, Location=Columbia, SC, Total=$512.00, Items: monitor, desk lamp, Discount: yes\",\n",
    "    \"Order=1018, Buyer=Jae Arora, Location=Dreher, FL, Total=$6.00, Items: chair, Discount: true, phone=123-456-789\",\n",
    "    \"Order=1019, Buyer=Jae Kao, Location=Atlanta, GA, Total=$12.00, Items: desk, Discount: False, phone=123-456-789\",\n",
    "    \"2026-01-30 14:22:01 INFO User login successful user_id=123\",\n",
    "    \"2026-01-30 14:22:01 INFO User login successful\",\n",
    "    \"level =INFO, user =Sam, id=1\",\n",
    "    \"timestamp=2026-01-30T14:22:01Z level=INFO user=alice action=login success=true\",\n",
    "    \"level=INFO cpu_usage=1,234.56 memory=512MB\",\n",
    "    '{\"level\":\"INFO\",\"service\":\"orders\",\"order_id\":1001,\"status\":\"created\"}',\n",
    "    '[2026-01-31 17:11:22 +0000] [7] [INFO] 127.0.0.1:56718 - - [31/Jan/2026:17:11:22 +0000] \"GET /health 1.1\" 200 16 \"-\" \"curl/8.14.1\"',\n",
    "    \"2026-01-31 17:11:00 swirl [DEBUG] saq_worker.py:28 Running cron job health check\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d865678",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be705a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "################################# Grammar Parsing ###############################\n",
    "#################################################################################\n",
    "\n",
    "string_batch = []\n",
    "string_json_batch = []\n",
    "for msg in messy_data:\n",
    "    if not (msg.startswith(\"[\") and msg.endswith(\"]\")) and not (\n",
    "        msg.startswith(\"{\") and msg.endswith(\"}\")\n",
    "    ):\n",
    "        string_batch.append(msg)\n",
    "    else:\n",
    "        string_json_batch.append(msg)\n",
    "\n",
    "print(f\"\\nUNSTRUCTURED STRING SAMPLES: {len(string_batch)}\\n\")\n",
    "print(f\"JSON STRING SAMPLES: {len(string_json_batch)}\\n\")\n",
    "\n",
    "\n",
    "string_samples = smart_parse_batch(string_batch)\n",
    "\n",
    "for i, (msg, parsed) in enumerate(string_samples):\n",
    "    print(f\"Original: {msg}\\nParsed: {parsed}\\n\")\n",
    "\n",
    "\n",
    "json_samples = []\n",
    "leftovers = []\n",
    "\n",
    "for msg in string_json_batch:\n",
    "    try:\n",
    "        data = json.loads(msg)\n",
    "        json_samples.append((msg, data))\n",
    "    except Exception:\n",
    "        leftovers.append((msg, None))\n",
    "\n",
    "\n",
    "data_samples = string_samples + json_samples\n",
    "\n",
    "print(f\"\\nTOTAL SAMPLES: {len(data_samples)}\\nERROR SAMPLES: {len(leftovers)}\\n\")\n",
    "\n",
    "#################################################################################\n",
    "############################### Structure Analyzer ##############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "analyzer = StructuralAnalyzer(ignore_unparsed=False)\n",
    "\n",
    "hash_counts = Counter()\n",
    "unique_structures = {}\n",
    "\n",
    "for raw, parsed in data_samples:\n",
    "    result = analyzer.generate_fingerprint(raw, parsed)\n",
    "    signature_hash = result[\"hash\"]\n",
    "    hash_counts[signature_hash] += 1\n",
    "    unique_structures[signature_hash] = unique_structures.get(signature_hash, result)\n",
    "\n",
    "print(\n",
    "    f\"Detected {len(unique_structures)} unique schemas across {len(data_samples)} records.\\n\"\n",
    ")\n",
    "\n",
    "for h, count in sorted(hash_counts.items()):\n",
    "    print(f\"Schema {h} ({count} occurrences):\")\n",
    "    print(f\"  Layout: {unique_structures[h]['signature']}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "############################# Structural Clustering #############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "def conjoin_signatures(registry_output: dict):\n",
    "    hashes = list(registry_output.keys())\n",
    "\n",
    "    signatures_as_text = [\n",
    "        \" \".join(registry_output[h][\"signature\"].keys()) for h in hashes\n",
    "    ]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5))\n",
    "    matrix = vectorizer.fit_transform(signatures_as_text)\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=2,\n",
    "        metric=\"euclidean\",\n",
    "        copy=True,\n",
    "    )\n",
    "    labels = clusterer.fit_predict(matrix.toarray())\n",
    "\n",
    "    conjoined_map = {}\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        h = hashes[i]\n",
    "        conjoined_map[h] = {\n",
    "            \"cluster_id\": int(cluster_id),\n",
    "            \"keys\": list(registry_output[h][\"signature\"].keys()),\n",
    "            \"is_outlier\": cluster_id == -1,\n",
    "        }\n",
    "\n",
    "    sorted_dict = dict(sorted(conjoined_map.items()))\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "############################## Semantic Clustering ##############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "def conjoin_signatures_semantic(\n",
    "    registry_output: dict,\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "    cache_dir: str = \"./.models\",\n",
    "):\n",
    "    hashes = list(registry_output.keys())\n",
    "    if not hashes:\n",
    "        return {}\n",
    "\n",
    "    signatures_as_text = []\n",
    "    for h in hashes:\n",
    "        h_dict = dict(registry_output[h][\"signature\"])\n",
    "        # remove the 'black hole' field that swallows everything\n",
    "        h_dict.pop(\"_unparsed\", None)\n",
    "\n",
    "        # sort keys to ensure structural identity regardless of log order\n",
    "        sorted_keys = sorted(h_dict.keys())\n",
    "\n",
    "        if not sorted_keys:\n",
    "            text_rep = \"schema:empty_blob\"\n",
    "        else:\n",
    "            # 'field:' prefix to define the role of the tokens\n",
    "            text_rep = \" \".join([f\"field:{k}\" for k in sorted_keys])\n",
    "\n",
    "        signatures_as_text.append(text_rep)\n",
    "\n",
    "    model = SentenceTransformer(embedding_model, cache_folder=cache_dir)\n",
    "    embeddings = model.encode(signatures_as_text)\n",
    "    X = np.ascontiguousarray(embeddings, dtype=np.float64)\n",
    "\n",
    "    clusterer = HDBSCAN(\n",
    "        min_cluster_size=2,\n",
    "        min_samples=1,\n",
    "        metric=\"cosine\",\n",
    "        cluster_selection_epsilon=0.08,\n",
    "        cluster_selection_method=\"eom\",\n",
    "        allow_single_cluster=True,\n",
    "        copy=True,\n",
    "    )\n",
    "\n",
    "    labels = clusterer.fit_predict(X)\n",
    "\n",
    "    conjoined_map = {}\n",
    "    for i, cluster_id in enumerate(labels):\n",
    "        h = hashes[i]\n",
    "        # unique IDs to outliers so they don't group into one '-1' bucket\n",
    "        final_id = int(cluster_id) if cluster_id != -1 else (400 + i)\n",
    "\n",
    "        conjoined_map[h] = {\n",
    "            \"cluster_id\": final_id,\n",
    "            \"keys\": list(registry_output[h][\"signature\"].keys()),\n",
    "            \"is_outlier\": cluster_id == -1,\n",
    "        }\n",
    "\n",
    "    return conjoined_map\n",
    "\n",
    "\n",
    "# run structure clustering\n",
    "structure_cluster_map = conjoin_signatures(analyzer.signature_map)\n",
    "structure_clusters = {}\n",
    "for k, v in structure_cluster_map.items():\n",
    "    cluster_id = v[\"cluster_id\"]\n",
    "    keys = v[\"keys\"]\n",
    "    is_outlier = bool(v[\"is_outlier\"])\n",
    "    structure_clusters[cluster_id] = structure_clusters.get(cluster_id, [])\n",
    "    structure_clusters[cluster_id].append(\n",
    "        {\"signature_hash\": k, \"fields\": keys, \"is_outlier\": is_outlier}\n",
    "    )\n",
    "print(f\"Structural Clusters: \\n{json.dumps(structure_clusters, indent=4)}\\n\")\n",
    "\n",
    "\n",
    "# run semantic clustering\n",
    "print(json.dumps(analyzer.signature_map, indent=4))\n",
    "semantic_cluster_map = conjoin_signatures_semantic(analyzer.signature_map)\n",
    "semantic_clusters = {}\n",
    "for k, v in semantic_cluster_map.items():\n",
    "    cluster_id = v[\"cluster_id\"]\n",
    "    keys = v[\"keys\"]\n",
    "    is_outlier = bool(v[\"is_outlier\"])\n",
    "    semantic_clusters[cluster_id] = semantic_clusters.get(cluster_id, [])\n",
    "    semantic_clusters[cluster_id].append(\n",
    "        {\"signature_hash\": k, \"fields\": keys, \"is_outlier\": is_outlier}\n",
    "    )\n",
    "print(f\"Semantic Clusters: \\n{json.dumps(semantic_clusters, indent=4)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb800c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic cluster map to raw string, rough parsed dict, structure cluster, structure signature hash\n",
    "cluster_dict = {}\n",
    "for cluster_id, records in semantic_clusters.items():\n",
    "    cluster_dict[cluster_id] = cluster_dict.get(cluster_id, [])\n",
    "    for rec in records:\n",
    "        signature_hash = rec[\"signature_hash\"]\n",
    "        analyzer_records = analyzer.signature_map[signature_hash][\"records\"]\n",
    "        fields_li = rec[\"fields\"]\n",
    "        # ignore _unparsed\n",
    "        fields_li = [f for f in fields_li if f != \"_unparsed\"]\n",
    "        if len(fields_li) < 1:\n",
    "            continue\n",
    "        for r in analyzer_records:\n",
    "            parsed_dict = r[\"parsed\"]\n",
    "            # ignore _unparsed\n",
    "            parsed_dict.pop(\"_unparsed\", None)\n",
    "            if len(parsed_dict) < 1:\n",
    "                continue\n",
    "            cluster_dict[cluster_id].append(\n",
    "                {\n",
    "                    \"signature_hash\": signature_hash,\n",
    "                    \"structure_cluster_id\": structure_cluster_map[signature_hash].get(\n",
    "                        \"cluster_id\"\n",
    "                    ),\n",
    "                    \"raw\": r[\"raw\"],\n",
    "                    \"parsed\": parsed_dict,\n",
    "                    \"fields\": fields_li,\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(json.dumps(cluster_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70354b4f",
   "metadata": {},
   "source": [
    "## LLM Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f369b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm connection\n",
    "API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "# api_base_url = os.getenv(\"LsLM_BASE_URL\")\n",
    "api_base_url = \"https://openrouter.ai/api/v1\"\n",
    "# model = \"openai/google/gemma-3-27b-it\"\n",
    "MODEL = \"openai/gpt-oss-120b:exacto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea7d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncLLMClient(\n",
    "    MODEL,\n",
    "    api_base_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc455a6a",
   "metadata": {},
   "source": [
    "## LLM Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f05a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "PYDANTIC_SYSTEM_PROMPT = \"\"\"You are a Data Architect. Your goal is to perform unsupervised schema inference on a sample of unstructured data.\n",
    "\n",
    "Generate a Pydantic `BaseModel` class that represents the \"Gold Standard\" foundation for this data pattern. \n",
    "\n",
    "Instructions:\n",
    "- Normalization: Suggest clean, snake_case keys for the identified fields.\n",
    "- If you see a string value for a field that follows a consistent structure (e.g., \"<city>, <state>\") then make sure that structure is accurately typed in the BaseModel.\n",
    "- Determine what fields should be required vs optional based on overall semantic meaning of the entity you are creating a BaseModel class for.\n",
    "\n",
    "Constraints:\n",
    "- Include a detailed description for each field using the `Field` class to explain what the field is and if there are any expected structural patterns (e.g., `state` should be two letters).\n",
    "- Create supplemental BaseModel classes where necessary to preserve semantic clarity.\n",
    "- Do NOT include any regex.\n",
    "- You MUST wrap your code in a python block with the following start marking \"```python\" and end marking \"```\".\n",
    "- If a field appears in some rows but not others, mark it as `Optional`.\n",
    "- You are only allowed to use the following imports: \"from typing import List, Dict, Optional; from pydantic import BaseModel, Field\".\n",
    "- Return ONLY the Pydantic class definitions (you are allowed to generate multiple as long as they are logically linked).\n",
    "\"\"\"\n",
    "\n",
    "PYDANTIC_USER_PROMPT = \"\"\"Please analyze the following representative samples of a new data pattern and generate the Pydantic 'Foundation' model.\n",
    "\n",
    "### Data Samples:\n",
    "{samples}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8c08fc",
   "metadata": {},
   "source": [
    "## Generate Pydantic BaseModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelResponseStructure(BaseModel):\n",
    "    code_string: str = Field(\n",
    "        ...,\n",
    "        description=\"generated python code\",\n",
    "    )\n",
    "    entrypoint_class_name: str = Field(\n",
    "        ...,\n",
    "        description=\"name of entrypoint base model class in the code generated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extracts the Python code block from a string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted source code or an empty string if not found.\n",
    "    \"\"\"\n",
    "    block_pattern = r\"```(?:python)?\\s*(.*?)\\s*```\"\n",
    "    match = re.search(block_pattern, text, re.DOTALL)\n",
    "\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "for c_id, records in cluster_dict.items():\n",
    "    string_li = [r[\"raw\"] for r in records]\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": PYDANTIC_SYSTEM_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": PYDANTIC_USER_PROMPT.format(\n",
    "                samples=string_li,\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "        response_format=ModelResponseStructure,\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    resp: ModelResponseStructure = ModelResponseStructure(**json.loads(resp))\n",
    "\n",
    "    if not resp.code_string.startswith(\"```python\"):\n",
    "        resp.code_string = f\"```python\\n{resp.code_string}\\n```\"\n",
    "\n",
    "    code = extract_python_code(resp.code_string)\n",
    "\n",
    "    namespace = {}\n",
    "    exec(code, globals(), namespace)\n",
    "\n",
    "    # access the function from the namespace dictionary\n",
    "    cls = namespace.get(resp.entrypoint_class_name)\n",
    "    cls.model_rebuild(_types_namespace=namespace)\n",
    "    schema = cls.model_json_schema()\n",
    "\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a80c7b",
   "metadata": {},
   "source": [
    "## Langgraph Robustness and Stategraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab909348",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECT_PROMPT = \"\"\"You are a Lead Data Architect.\n",
    "Define a simple Pydantic v2 `BaseModel` that represents the \"Gold Standard\" foundation for the data pattern found in the input samples.\n",
    "\n",
    "INPUT SAMPLES (Multiple variations):\n",
    "{samples}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Normalization: Suggest clean, snake_case keys for the identified fields.\n",
    "2. Determine what fields should be required vs optional based on overall semantic meaning of the entity you are creating a BaseModel class for.\n",
    "3. Include a detailed description for each field using the `Field` class to explain what the field is and if there are any expected structural patterns (e.g., `state` should be two letters).\n",
    "4. Do NOT include any regex.\n",
    "5. You MUST wrap your code in a python block with the following start marking \"```python\" and end marking \"```\".\n",
    "6. Create supplemental BaseModel classes where necessary to preserve semantic clarity.\n",
    "7. You are ONLY allowed to use the following imports: \"from typing import List, Dict, Optional; from pydantic import BaseModel, Field\".\n",
    "8. Keep primary keys as type string.\n",
    "9. Infer best data type from string value (e.g., money should be a float, \"true/false\" or \"yes/no\" fields should be a boolean, and fields that represent multiple entities should use a representative aggregate data structure type)\n",
    "10. NEVER set potentially boolean fields as optional. Instead, when not explicitly declared, infer as to what the default value ought based on the semantic meaning of the field and how it appears in the samples that do provide it.\n",
    "11. Perform semantic merging: Identify fields across structural variants that share the same intent and conjoin them under a single, definitive schema key to avoid redundancy (e.g., \"location\" vs \"city\", \"state\", \"zip code\")\n",
    "12. Avoid information loss when it comes to key:value pairs in the sample data.\n",
    "\n",
    "Return ONLY the Python code for the class. Include necessary imports (from pydantic import BaseModel, Field, etc.).\n",
    "\"\"\"\n",
    "\n",
    "CODER_PROMPT = \"\"\"You are a Senior Data Engineer.\n",
    "Your task is to write a concise but effective transformation function `transform_to_models(parsed_dict: list[dict]) -> list[dict]` that maps roughly parsed dictionaries into the provided pydantic v2 target schema base model definition.\n",
    "\n",
    "TARGET SCHEMA (Python Pydantic v2 BaseModel):\n",
    "{schema}\n",
    "\n",
    "SOURCE SAMPLES:\n",
    "{samples}\n",
    "\n",
    "Logic Requirements:\n",
    "1. Use a 'coalesce' approach: for each target field, check all possible source keys from the input dictionary samples.\n",
    "2. The Target Schema is the gold standard so ensure that the transformation function maps and casts the data types of the input appropriately.\n",
    "3. Use parsed_dict.get() for optional fields.\n",
    "4. Infer best data type from string (e.g., \"$120.00\" should be a float, and \"true\" should be a boolean). \n",
    "5. ALL python code must be encapsulated by the `transform_to_models()` function -- if it's not in that function it will not be run.\n",
    "\n",
    "Return ONLY the Python code for the function `transform_to_models`. Do not include the Pydantic class in your response.\n",
    "\"\"\"\n",
    "\n",
    "CODE_EXECUTION = \"\"\"\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import *\n",
    "import json, re\n",
    "\n",
    "{schema}\n",
    "\n",
    "{parser_code}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a80d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import operator\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Annotated, Any, Dict, List, Literal, Optional, TypedDict\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from dq_swirl.utils.log_utils import get_custom_logger\n",
    "\n",
    "logger = get_custom_logger()\n",
    "\n",
    "\n",
    "def extract_python_code(text):\n",
    "    \"\"\"\n",
    "    Extracts the Python code block from a string.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted source code or an empty string if not found.\n",
    "    \"\"\"\n",
    "    block_pattern = r\"```(?:python)?\\s*(.*?)\\s*```\"\n",
    "    match = re.search(block_pattern, text, re.DOTALL)\n",
    "\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "\n",
    "class ModelResponseStructure(BaseModel):\n",
    "    code_string: str = Field(\n",
    "        ...,\n",
    "        description=\"generated python code\",\n",
    "    )\n",
    "    entrypoint_class_name: str = Field(\n",
    "        ...,\n",
    "        description=\"name of entrypoint base model class in the code generated\",\n",
    "    )\n",
    "\n",
    "\n",
    "class MultiAgentState(TypedDict):\n",
    "    semantic_id: str\n",
    "    structure_cluster_id: str\n",
    "    data_pairs_all: List[Dict[str, Any]]\n",
    "    data_pairs_structure: List[Dict[str, Any]]\n",
    "    gold_schema: Annotated[Optional[ModelResponseStructure], lambda old, new: new]\n",
    "    parser_code: Annotated[Optional[str], lambda old, new: new]\n",
    "    feedback: Annotated[Optional[str], lambda old, new: new]\n",
    "    error_type: Annotated[\n",
    "        Optional[Literal[\"SCHEMA_ISSUE\", \"CODE_ISSUE\"]], lambda old, new: new\n",
    "    ]\n",
    "    attempts: Annotated[int, operator.add]  # increment\n",
    "    export_map: Annotated[Optional[Dict[str, Any]], lambda old, new: new]\n",
    "\n",
    "\n",
    "async def architect_node(state: MultiAgentState) -> Dict[str, Any]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    :param state: _description_\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "    if state.get(\"gold_schema\") and state.get(\"error_type\") != \"SCHEMA_ISSUE\":\n",
    "        return {\"attempts\": 0}\n",
    "\n",
    "    logger.info(f\"[Architect] Defining Semantic Goal: {state['attempts']}\")\n",
    "\n",
    "    # diversity is key to generalize\n",
    "    samples = json.dumps([p[\"parsed\"] for p in state[\"data_pairs_all\"][:100]], indent=2)\n",
    "\n",
    "    logger.debug(samples)\n",
    "\n",
    "    prompt = ARCHITECT_PROMPT.format(\n",
    "        samples=samples,\n",
    "    )\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        api_key_override=API_KEY,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "        response_format=ModelResponseStructure,\n",
    "        num_retries=5,\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    resp = ModelResponseStructure(**json.loads(resp))\n",
    "    resp.code_string = extract_python_code(resp.code_string)\n",
    "    print(resp.code_string)\n",
    "\n",
    "    return {\n",
    "        \"gold_schema\": resp,\n",
    "        \"attempts\": 1,\n",
    "        \"feedback\": None,\n",
    "        \"error_type\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "async def schema_tester_node(state: MultiAgentState) -> Dict[str, Any]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    :param state: _description_\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "    logger.info(f\"[Scehma Tester] Validating Functional BaseModel: {state['attempts']}\")\n",
    "    python_base_model_str = state[\"gold_schema\"].code_string\n",
    "\n",
    "    env = {}\n",
    "    try:\n",
    "        exec(python_base_model_str, globals(), env)\n",
    "        cls_name = state[\"gold_schema\"].entrypoint_class_name\n",
    "        model = env[cls_name]\n",
    "\n",
    "        model.model_rebuild(_types_namespace=env)\n",
    "        _ = model.model_json_schema()\n",
    "\n",
    "        return {\"feedback\": \"SUCCESS\"}\n",
    "    except Exception as e:\n",
    "        err_msg = traceback.format_exc()\n",
    "        logger.exception(e)\n",
    "        return {\n",
    "            \"feedback\": err_msg,\n",
    "            \"error_type\": \"SCHEMA_ISSUE\",\n",
    "        }\n",
    "\n",
    "\n",
    "async def coder_node(state: MultiAgentState) -> Dict[str, Any]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    :param state: _description_\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "    logger.info(f\"[Coder] Parser for Gold Schema: {state['attempts']}\")\n",
    "    samples = json.dumps(\n",
    "        [rec[\"parsed\"] for rec in state[\"data_pairs_structure\"]], indent=2\n",
    "    )\n",
    "\n",
    "    prompt = CODER_PROMPT.format(\n",
    "        schema=state[\"gold_schema\"].code_string, samples=samples\n",
    "    )\n",
    "    buffer = []\n",
    "    response = await client.chat(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        api_key_override=API_KEY,\n",
    "        stream=True,\n",
    "        temperature=0.0,\n",
    "        num_retries=5,\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            print(content, end=\"\", flush=True)\n",
    "            buffer.append(content)\n",
    "\n",
    "    resp = \"\".join(buffer)\n",
    "    code = extract_python_code(resp)\n",
    "\n",
    "    return {\n",
    "        \"parser_code\": code,\n",
    "        \"attempts\": 1,\n",
    "        \"feedback\": None,\n",
    "        \"error_type\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "async def code_tester_node(state: MultiAgentState) -> Dict[str, Any]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    :param state: _description_\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "    logger.info(f\"[Code Tester] Stress-testing parser: {state['attempts']}\")\n",
    "    full_code = CODE_EXECUTION.format(\n",
    "        schema=state[\"gold_schema\"].code_string,\n",
    "        parser_code=state[\"parser_code\"],\n",
    "    )\n",
    "\n",
    "    env = {}\n",
    "    try:\n",
    "        cls_name = state[\"gold_schema\"].entrypoint_class_name\n",
    "        exec(full_code, globals(), env)\n",
    "        func = env[\"transform_to_models\"]\n",
    "        model = env[cls_name]\n",
    "        model.model_rebuild(_types_namespace=env)\n",
    "\n",
    "        input_data = [pair[\"parsed\"] for pair in state[\"data_pairs_structure\"]]\n",
    "        mapped_batch = func(input_data)\n",
    "        for mapped_dict in mapped_batch:\n",
    "            model.model_validate(mapped_dict)\n",
    "            logger.debug(f\"Input: {mapped_dict} -- PASSED\")\n",
    "        return {\n",
    "            \"feedback\": \"SUCCESS\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        err_msg = traceback.format_exc()\n",
    "        try:\n",
    "            logger.debug(f\"Input: {mapped_dict} -- FAILED\")\n",
    "            logger.exception(e)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return {\n",
    "            \"feedback\": err_msg,\n",
    "            \"error_type\": \"CODE_ISSUE\",\n",
    "        }\n",
    "\n",
    "\n",
    "async def exporter_node(state: MultiAgentState) -> Dict[str, Any]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    :param state: _description_\n",
    "    :return: _description_\n",
    "    \"\"\"\n",
    "    # get necesary fields\n",
    "    semantic_cluster_id = state[\"semantic_id\"]\n",
    "    structure_cluster_id = state[\"structure_cluster_id\"]\n",
    "    data_pairs_structure = state[\"data_pairs_structure\"]\n",
    "    base_model_name = state[\"gold_schema\"].entrypoint_class_name.lower()\n",
    "\n",
    "    base_model_code_str = state[\"gold_schema\"].code_string\n",
    "    parser_code_str = state[\"parser_code\"]\n",
    "    export_map = state.get(\"export_map\", {})\n",
    "\n",
    "    # make the directory\n",
    "    dir_name = f\"sem_{semantic_cluster_id}-{base_model_name}\"\n",
    "    dir_path = Path(dir_name)\n",
    "    dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # write the base model\n",
    "    base_model_fpath = os.path.join(dir_path, f\"{base_model_name}_base_model.py\")\n",
    "    with open(base_model_fpath, \"w\") as f:\n",
    "        f.write(base_model_code_str)\n",
    "\n",
    "    logger.info(f\"Exported: {base_model_fpath}\")\n",
    "\n",
    "    # write parser\n",
    "    parser_fpath = os.path.join(\n",
    "        dir_path,\n",
    "        f\"{base_model_name}_parser-struct_{structure_cluster_id}.py\",\n",
    "    )\n",
    "    with open(parser_fpath, \"w\") as f:\n",
    "        f.write(parser_code_str)\n",
    "\n",
    "    logger.info(f\"Exported: {parser_fpath}\")\n",
    "\n",
    "    # update export map\n",
    "    struct_records = [\n",
    "        {\n",
    "            rec[\"signature_hash\"]: rec[\"fields\"],\n",
    "        }\n",
    "        for rec in data_pairs_structure\n",
    "    ]\n",
    "    export_map[semantic_cluster_id] = export_map.get(semantic_cluster_id, {})\n",
    "    export_map[semantic_cluster_id][\"base_model_fpath\"] = export_map[\n",
    "        semantic_cluster_id\n",
    "    ].get(\"base_model_fpath\", base_model_fpath)\n",
    "    export_map[semantic_cluster_id][\"structure_clusters\"] = export_map[\n",
    "        semantic_cluster_id\n",
    "    ].get(\"structure_clusters\", [])\n",
    "    export_map[semantic_cluster_id][\"structure_clusters\"].append(\n",
    "        {\n",
    "            \"id\": structure_cluster_id,\n",
    "            \"struct_records\": struct_records,\n",
    "            \"parser_fpath\": parser_fpath,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"feedback\": \"DONE\",\n",
    "        \"export_map\": export_map,\n",
    "    }\n",
    "\n",
    "\n",
    "def schema_router(state: MultiAgentState) -> str:\n",
    "    \"\"\"Determines if we move to Coder or retry the Architect\"\"\"\n",
    "    feedback = state.get(\"feedback\")\n",
    "    attempts = state.get(\"attempts\", 0)\n",
    "\n",
    "    if feedback == \"SUCCESS\":\n",
    "        return \"coder\"\n",
    "\n",
    "    # if failed too many times, just stop the process\n",
    "    if attempts >= 3:\n",
    "        logger.error(f\"Schema failed after {attempts} attempts. Aborting.\")\n",
    "        return \"end\"\n",
    "\n",
    "    return \"architect\"\n",
    "\n",
    "\n",
    "def code_router(state: MultiAgentState) -> str:\n",
    "    \"\"\"Determines if we export or retry Coder/Architect.\"\"\"\n",
    "    feedback = state.get(\"feedback\")\n",
    "    error_type = state.get(\"error_type\")\n",
    "    attempts = state.get(\"attempts\", 0)\n",
    "\n",
    "    if feedback == \"SUCCESS\":\n",
    "        return \"exporter\"\n",
    "\n",
    "    if attempts >= 6:\n",
    "        return \"end\"\n",
    "\n",
    "    # Specific routing based on where the failure happened\n",
    "    if error_type == \"SCHEMA_ISSUE\":\n",
    "        return \"architect\"\n",
    "\n",
    "    # Default to retrying the coder for CODE_ISSUE or unknown errors\n",
    "    return \"coder\"\n",
    "\n",
    "\n",
    "## Define Graph\n",
    "workflow = StateGraph(MultiAgentState)\n",
    "workflow.add_node(\"architect\", architect_node)\n",
    "workflow.add_node(\"schema_tester\", schema_tester_node)\n",
    "workflow.add_node(\"coder\", coder_node)\n",
    "workflow.add_node(\"code_tester\", code_tester_node)\n",
    "workflow.add_node(\"exporter\", exporter_node)\n",
    "\n",
    "workflow.add_edge(START, \"architect\")\n",
    "workflow.add_edge(\"architect\", \"schema_tester\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"schema_tester\",\n",
    "    schema_router,\n",
    "    {\n",
    "        \"architect\": \"architect\",\n",
    "        \"coder\": \"coder\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"coder\", \"code_tester\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"code_tester\",\n",
    "    code_router,\n",
    "    {\n",
    "        \"architect\": \"architect\",\n",
    "        \"coder\": \"coder\",\n",
    "        \"exporter\": \"exporter\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"exporter\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_data_matrix(all_data: Dict[int, List[Dict]]) -> Dict[str, Any]:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    :param all_data: _description_\n",
    "    \"\"\"\n",
    "    shared_export_map = {}\n",
    "\n",
    "    for sem_id, records in all_data.items():\n",
    "        struct_groups = {}\n",
    "        for s in records:\n",
    "            cid = s[\"structure_cluster_id\"]\n",
    "            struct_groups.setdefault(cid, []).append(s)\n",
    "\n",
    "        shared_gold_schema = None\n",
    "\n",
    "        for struct_id, pairs in struct_groups.items():\n",
    "            config = {\n",
    "                \"configurable\": {\n",
    "                    \"thread_id\": f\"sem_{sem_id}_str_{struct_id}\",\n",
    "                }\n",
    "            }\n",
    "\n",
    "            initial_state = {\n",
    "                \"semantic_id\": str(sem_id),\n",
    "                \"structure_cluster_id\": str(struct_id),\n",
    "                \"data_pairs_all\": records,\n",
    "                \"data_pairs_structure\": pairs,\n",
    "                \"gold_schema\": shared_gold_schema,\n",
    "                \"export_map\": shared_export_map,\n",
    "                \"feedback\": None,\n",
    "                \"attempts\": 0,\n",
    "            }\n",
    "\n",
    "            final_output = await app.ainvoke(initial_state, config)\n",
    "            shared_gold_schema = final_output.get(\"gold_schema\")\n",
    "            shared_export_map = final_output.get(\"export_map\")\n",
    "            logger.info(\n",
    "                f\"--- Finished Sem{sem_id}-Struct{struct_id} ---\\n{json.dumps(shared_export_map, indent=4)}\"\n",
    "            )\n",
    "\n",
    "    return shared_export_map\n",
    "\n",
    "\n",
    "code_export_map = await run_data_matrix(cluster_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36ef94",
   "metadata": {},
   "source": [
    "## Testing Generated Parsers on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64494fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(code_export_map, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_messy_data = [\n",
    "    \"Order=2011, Buyer=Gemma Claude, Location=Atlanta, GA, Total=$1,356.00, Items: hairpin, wallet, keys, Discount: True, phone=123-456-789\",\n",
    "    \"Order 2005: Buyer=Anthropic Google, Location=Austin, TX, Total=$213,00,00.55, Items: headphones\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get map of structure signature -> code\n",
    "signature_map = {}\n",
    "for semantic_cluster_id, export_dict in code_export_map.items():\n",
    "    base_model_fpath = export_dict[\"base_model_fpath\"]\n",
    "    structure_clusters = export_dict[\"structure_clusters\"]\n",
    "    for cluster_dict in structure_clusters:\n",
    "        structure_id = cluster_dict[\"id\"]\n",
    "        structure_records = cluster_dict[\"struct_records\"]\n",
    "        parser_fpath = cluster_dict[\"parser_fpath\"]\n",
    "        for struct_dict in structure_records:\n",
    "            signature_hash = list(struct_dict.keys())[0]\n",
    "            fields = struct_dict[signature_hash]\n",
    "            signature_map[signature_hash] = signature_map.get(signature_hash, {})\n",
    "            signature_map[signature_hash][\"base_model_fpath\"] = signature_map[\n",
    "                signature_hash\n",
    "            ].get(\"base_model_fpath\", base_model_fpath)\n",
    "            signature_map[signature_hash][\"parser_fpath\"] = signature_map[\n",
    "                signature_hash\n",
    "            ].get(\"parser_fpath\", parser_fpath)\n",
    "            signature_map[signature_hash][\"fields\"] = signature_map[signature_hash].get(\n",
    "                \"fields\", fields\n",
    "            )\n",
    "\n",
    "\n",
    "signature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_data = unseen_messy_data\n",
    "\n",
    "#################################################################################\n",
    "################################# Grammar Parsing ###############################\n",
    "#################################################################################\n",
    "\n",
    "string_batch = []\n",
    "string_json_batch = []\n",
    "for msg in messy_data:\n",
    "    if not (msg.startswith(\"[\") and msg.endswith(\"]\")) and not (\n",
    "        msg.startswith(\"{\") and msg.endswith(\"}\")\n",
    "    ):\n",
    "        string_batch.append(msg)\n",
    "    else:\n",
    "        string_json_batch.append(msg)\n",
    "\n",
    "print(f\"\\nUNSTRUCTURED STRING SAMPLES: {len(string_batch)}\\n\")\n",
    "print(f\"JSON STRING SAMPLES: {len(string_json_batch)}\\n\")\n",
    "\n",
    "\n",
    "string_samples = smart_parse_batch(string_batch)\n",
    "\n",
    "for i, (msg, parsed) in enumerate(string_samples):\n",
    "    print(f\"Original: {msg}\\nParsed: {parsed}\\n\")\n",
    "\n",
    "\n",
    "json_samples = []\n",
    "leftovers = []\n",
    "\n",
    "for msg in string_json_batch:\n",
    "    try:\n",
    "        data = json.loads(msg)\n",
    "        json_samples.append((msg, data))\n",
    "    except Exception:\n",
    "        leftovers.append((msg, None))\n",
    "\n",
    "\n",
    "data_samples = string_samples + json_samples\n",
    "\n",
    "print(f\"\\nTOTAL SAMPLES: {len(data_samples)}\\nERROR SAMPLES: {len(leftovers)}\\n\")\n",
    "\n",
    "#################################################################################\n",
    "############################### Structure Analyzer ##############################\n",
    "#################################################################################\n",
    "\n",
    "\n",
    "new_analyzer = StructuralAnalyzer(ignore_unparsed=False)\n",
    "\n",
    "hash_counts = Counter()\n",
    "unique_structures = {}\n",
    "\n",
    "for raw, parsed in data_samples:\n",
    "    result = new_analyzer.generate_fingerprint(raw, parsed)\n",
    "    signature_hash = result[\"hash\"]\n",
    "    hash_counts[signature_hash] += 1\n",
    "    unique_structures[signature_hash] = unique_structures.get(signature_hash, result)\n",
    "\n",
    "print(\n",
    "    f\"Detected {len(unique_structures)} unique schemas across {len(data_samples)} records.\\n\"\n",
    ")\n",
    "\n",
    "for h, count in sorted(hash_counts.items()):\n",
    "    print(f\"Schema {h} ({count} occurrences):\")\n",
    "    print(f\"  Layout: {unique_structures[h]['signature']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a255852",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hash_signature, records in new_analyzer.signature_map.items():\n",
    "    try:\n",
    "        hash_dict = signature_map[hash_signature]\n",
    "        parsed_records = [r[\"parsed\"] for r in records[\"records\"]]\n",
    "        logger.debug(f\"Parsed Records: {parsed_records}\")\n",
    "        logger.debug(f\"Found Hash Dict: {hash_dict}\")\n",
    "    except KeyError:\n",
    "        logger.exception(f\"Hash Signature: {hash_signature} Does Not Exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_function(file_path, function_name) -> callable:\n",
    "    # 1. Convert string path to a Path object for easier handling\n",
    "    path = Path(file_path)\n",
    "\n",
    "    # 2. Give it a placeholder name for the session\n",
    "    module_name = path.stem  # This uses the filename without .py\n",
    "\n",
    "    # 3. Create a 'spec' (the blueprint for the module)\n",
    "    spec = importlib.util.spec_from_file_location(module_name, str(path))\n",
    "\n",
    "    # 4. Create the actual module object from that spec\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "\n",
    "    # 5. Execute the module so the functions actually exist\n",
    "    spec.loader.exec_module(module)\n",
    "\n",
    "    # 6. Get the function from the module and call it\n",
    "    func = getattr(module, function_name)\n",
    "    return func\n",
    "\n",
    "\n",
    "for hash_signature, records in new_analyzer.signature_map.items():\n",
    "    hash_dict = signature_map[hash_signature]\n",
    "    function_fpath = hash_dict[\"parser_fpath\"]\n",
    "    transform_func = load_function(function_fpath, \"transform_to_models\")\n",
    "    raw_recrods = [r[\"raw\"] for r in records[\"records\"]]\n",
    "    parsed_records = [r[\"parsed\"] for r in records[\"records\"]]\n",
    "    logger.debug(f\"BEFORE: {json.dumps(raw_recrods, indent=2)}\")\n",
    "    result = transform_func(parsed_records)\n",
    "    logger.debug(f\"AFTER: {json.dumps(result, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4534489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dq-swirl (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
